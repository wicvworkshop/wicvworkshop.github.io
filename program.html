
<!DOCTYPE HTML>


<!--
 Berkeley Vision and Learning Center (BVLC)
 
 Design based on:
 Strongly Typed 1.0 by HTML5 UP
 html5up.net | @n33co
 Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
 -->
<html>
    <head>
        <title>WiCV</title>
        <meta http-equiv="content-type" content="text/html; charset=utf-8" />
        <meta name="description" content="" />
        <meta name="keywords" content="" />
        <meta name="viewport" content="width=1040" />
        <link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600|Arvo:700" rel="stylesheet" type="text/css" />
        <!--[if lte IE 8]><script src="js/html5shiv.js"></script><![endif]-->
        <script src="js/jquery.min.js"></script>
        <script src="js/jquery.dropotron.js"></script>
        <script src="js/jquery.dotdotdot.min.js"></script>
        <script src="js/config.js"></script>
        <script src="js/skel.min.js"></script>
        <script src="js/skel-panels.min.js"></script>
        <script src="js/jquery.slides.min.js"></script>
        <link rel="stylesheet" href="css/font-awesome.min.css">
            <noscript>
                <link rel="stylesheet" href="css/style.css" />
                <link rel="stylesheet" href="css/style-desktop.css" />
                <link rel="stylesheet" href="css/skel-noscript.css" />
            </noscript>
        
        <style>
        
        /* Style the tab */
        div.tab {
            margin: auto;
            width: 75%;
            padding: 0px 12px; 
            overflow: hidden;
            border: 1px solid #ccc;
            background-color: #f1f1f1;
        }
        
        /* Style the buttons inside the tab */
        div.tab button {
            background-color: inherit;
            float: left;
            border: none;
            outline: none;
            cursor: pointer;
            padding: 14px 16px;
            transition: 0.3s;
            font-size: 17px;
        }
        
        /* Change background color of buttons on hover */
        div.tab button:hover {
            background-color: #ddd;
        }
        
        /* Create an active/current tablink class */
        div.tab button.active {
            background-color: #ccc;
        }
        
        /* Style the tab content */
        .tabcontent {
            margin: auto;
            width: 75%;
            display: none;
            padding: 6px 12px;
            border: 1px solid #ccc;
            border-top: none;
        }
        </style>

            </head>
    <body class="homepage">
        <!-- Header Wrapper -->
        <div id="header-wrapper">
            <!-- Header -->
            <div id="header" class="container">
                <!-- Logo -->
                <!--<h1 id="logo">
                 <a id="home" href="#"></a>
                 </h1>-->
                <!-- Nav -->
                <nav id="nav">
                    <ul style="padding-top: 10px; padding-bottom: 2em">
                        <li>
                            <a href="index.html#header" class="">
                                <span style="
                                    position: relative;
                                    width: 261px;
                                    display: inline-block;
                                    height: 10px;
                                    ">
                                    <img src="images/wicv_logo_simple.png" style="
                                    position: absolute;
                                    left: 0;
                                    width: 111px;
                                    top: -20px;
                                    "/>
                                </span>
                            </a>
                        </li>
                        
                        <li>
                            <a href="index.html#header"><span>Home</span></a>
                        </li>
                        <li>
                            <a href="program.html"><span>Program</span></a>
                        </li>
                        <li>
                            <a href="faq.html"><span>FAQ</span></a>
                        </li>
                        <li>
                            <a href="committee.html"><span>Committee</span></a>
                        </li>
                        <li>
                            <a href="participation.html"><span>Call for Participation</span></a>
                        </li>
                        <li>
                            <a href="contact.html"><span>Contact</span></a>
                            
                        </li>
                        <li>
                           <a href="#"><span>WiCV</span></a>
                           <ul>
                              <li><a href="https://sites.google.com/site/wicv2016/home">WiCV 2016</a></li>
                              <li><a href="https://sites.google.com/site/wicv2015/home">WiCV 2015</a></li>
                           </ul>
                        </li>
                    </ul>
                </nav>
            </div>
        </div>
        
        <div class="banner-wrapper">
            <div class="inner">
                <!-- Banner -->
                <section class="banner container">
                    <br>
                    <h2 id="faculty">Program</h2>
                    
                </section>
            </div>
        </div>
        <div class="features-wrapper">
                  
        <p style="color:red;"><strong>* Check back later for the final schedule. * </strong></p>

        <div class="tab">
          <button class="tablinks" onclick="openCity(event, 'Schedule')" id="defaultOpen">Schedule</button>
          <button class="tablinks" onclick="openCity(event, 'Talks')">Talks</button>
          <button class="tablinks" onclick="openCity(event, 'Panel')">Panel</button>
          <button class="tablinks" onclick="openCity(event, 'Orals')">Orals</button>
          <button class="tablinks" onclick="openCity(event, 'Posters')">Posters</button>
          <button class="tablinks" onclick="openCity(event, 'Dinner')">Dinner</button>
        </div>
        
        <div id="Schedule" class="tabcontent">
          <h3 style="margin: 20px 0px 0px 0px;">Main Workshop on July 26</h3>
          <!--<h5 style="color:red;font-style:italic; margin: 0px 0px 20px 0px;">tentative schedule</h5> -->
          <p style="margin: 0px 0px 20px 40px;text-align:left;">
                1:30 - 1:40 pm &emsp;&emsp;<b>Introduction</b> <br>
                1:40 - 2:00 pm &emsp;&emsp;<b>Keynote</b><br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Learning to Segment Moving Objects, by <a href="http://thoth.inrialpes.fr/~schmid/">Cordelia Schmid (INRIA)</a> <br>
                2:00 - 2:30 pm &emsp;&emsp;<b>Oral Session 1</b><br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Gaze Embeddings for Zero-Shot Image Classification<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;by Nour Karessli (Max Planck Institute for Informatics)<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Towards Better Instance-level Recognition<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;by Georgia Gkioxari (Facebook AI Research)<br>
                2:30 - 2:50 pm &emsp;&emsp;<b>Keynote</b><br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Interferences in Match Kernels, by <a href="http://www.xrce.xerox.com/About-XRCE/People/Naila-Murray">Naila Murray (Naver Labs/NLE)</a><br>
                2:50 - 4:15 pm &emsp;&emsp;<b>Poster Session and Coffee Break</b><br>
                4:15 - 4:35 pm &emsp;&emsp;<b>Keynote</b><br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Computer Vision for the Blind, by <a href="http://researcher.watson.ibm.com/researcher/view.php?person=jp-CHIE">Chieko Asakawa (IBM Research/CMU)</a>  <br>
                4:35 - 5:05 pm &emsp;&emsp;<b>Oral Session 2</b> <br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<a href="https://arxiv.org/pdf/1701.00299.pdf">Dynamic Deep Neural Networks: Optimizing Accuracy-Efficiency Trade-offs by Selective Execution</a><br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;by Lanlan Liu (University of Michigan)<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<a href="https://arxiv.org/abs/1703.09695">Semi and Weakly Supervised Semantic Segmentation Using Generative Adversarial Network</a><br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;by <a href="http://www.nasimsouly.com/">Nasim Souly</a> (University of Central Florida)<br>
                5:05 - 5:35 pm &emsp;&emsp;<b>Panel session</b><br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<a href="http://researcher.watson.ibm.com/researcher/view.php?person=jp-CHIE">Chieko Asakawa (IBM Research/CMU)</a><br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<a  href="https://clarifai.com/">Andrea Frome (Clarifai)</a><br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<a href="http://raiahadsell.com/index.html">Raia Hadsell (DeepMind)</a><br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<a href="http://www.xrce.xerox.com/About-XRCE/People/Naila-Murray">Naila Murray (Naver Labs/NLE)</a><br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<a href="http://thoth.inrialpes.fr/~schmid/">Cordelia Schmid (INRIA)</a><br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<a href="http://www.tandemlaunch.com">Helge Seetzen (TandemLaunch)</a><br>
                5:35 - 5:45 pm &emsp;&emsp;<b>Closing Remarks</b> <br>
          </p>

        </div>
        
        <div id="Talks" class="tabcontent">
          <h2 style="margin: 20px 0px 0px 0px;">Keynote Talks</h2>
          <p>Keynote speakers will give technical talks about their research in computer vision.</p> 


            <!-- Chieko Asakawa --> 
            <div class="row">
            <div class="4u"> 
                <a href="http://researcher.watson.ibm.com/researcher/view.php?person=jp-CHIE" class="image image-centered"><img height=230 width=120 src="images/speakers/chiekoasakawa.png" alt="" /></a>
                <strong>Chieko Asakawa (IBM Research/CMU)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                     <b>Title:</b> Computer Vision for the Blind
                     <br><br>
                     <b>Abstract:</b> Blind people have been dreaming of a machine which can recognize objects, people and environment, such as goods in a shop, people around them, or obstacles in a corridor. For many years, such machines were only available in science fiction, but now thanks to the advancement of deep learning and computer vision technologies, it is becoming closer to a reality, by supplementing and augmenting missing or weakened abilities of people with visual impairments. In this talk, I will outline a set of necessary technologies, demonstrate our efforts and cast a vision for near future deployments to change people's lives.
                     <br><br>
                     <b>Bio:</b> Chieko Asakawa is a blind Japanese computer scientist, known for her work at IBM Research – Tokyo in accessibility. A Netscape browser plug-in which she developed, the IBM Home Page Reader, became the most widely used web-to-speech system available. She is the recipient of numerous industry and government awards. Asakawa was born with normal sight, but after she injured her optic nerve in a swimming accident at age 11, she began losing her sight, and by age 14 she was fully blind. She earned a bachelor's degree in English literature at Otemon Gakuin University in Osaka in 1982 and then began a two-year computer programming course for blind people using an Optacon to translate print to tactile sensation. Chieko joined IBM in 1985 after completing the computer science courses for the blind at Nippon Lighthouse. She received a B.A. degree in English literature from Otemon Gakuin University in 1982, and a Ph.D in Engineering from the University of Tokyo in 2004. She is a member of the Association for Computing Machinery (ACM), the Information Processing Society of Japan, and IBM Academy of Technology. She was inducted into the Women in Technology International (WITI) Hall of Fame in 2003, and both within and outside of IBM, she has been actively working to help women engineers pursue technical careers. Chieko was appointed to IBM Fellow in 2009, IBM's most prestigious technical honor. In 2013, the government of Japan awarded the Medal of Honor with Purple Ribbon to Chieko for her outstanding contributions to accessibility research, including the development of the voice browser for the visually impaired.
                  </p> 
              </div>
            </div>          

            <!-- Naila Murray --> 
            <div class="row">
              <div class="4u"> 
                  <a href="http://www.xrce.xerox.com/About-XRCE/People/Naila-Murray" class="image image-centered"><img height=230 width=120 src="images/speakers/nailamurray.jpg" alt="" /></a>
                  <strong>Naila Murray (Naver Labs/NLE)</strong>
              </div>    
              <div class="8u">         
              <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                  <b>Title:</b> Interferences in Match Kernels
                  <br><br>
                  <b>Abstract:</b> We  consider  the  design  of  an  image  representation that embeds and aggregates a set of local descriptors into a  single  vector.  Popular  representations  of  this  kind  include the  bag-of-visual-words,  the  Fisher  vector  and  the  VLAD. When two such image representations are compared with the dot-product, the image-to-image similarity can be interpreted as  a  match  kernel.  In  match  kernels,  one  has  to  deal  with interference, i.e. with the fact that even if two descriptors are unrelated, their matching score may contribute to the overall similarity. We  formalise  this  problem  and  propose  two  related  solutions,  both  aimed  at  equalising  the  individual  contributions of  the  local  descriptors  in  the  final  representation.  These methods  modify  the  aggregation  stage  by  including  a  set  of per-descriptor  weights.  They  differ  by  the  objective  function that  is  optimised  to  compute  those  weights.  The  first  is  a “democratisation” strategy that aims at equalising the relative importance  of  each  descriptor  in  the  set  comparison  metric. The  second  one  involves  equalising  the  match  of  a  single descriptor  to  the  aggregated  vector. These  concurrent  methods  give  a  substantial  performance boost  over  standard aggregation methods,  as  demonstrated  by  our  experiments  on standard  public  image retrieval  benchmarks.
                  <br><br>
                  <b>Bio:</b> Naila Murray graduated with a PhD in computer science from the Unversitat Autònoma de Barcelona. She also holds a master’s degree in computer vision and artificial intelligence from the Unversitat Autònoma de Barcelona, and a bachelor’s degree in electrical engineering from Princeton University. Her work in computer vision has involved research into biologically-inspired deep models of visual attention; fine-grained visual recognition, including participation in the winning team of the FGComp 2013 competition; and  computational models for visual aesthetic analysis. Naila is a senior scientist and manager of the computer vision group at Xerox Research Centre Europe. Currently, her research focuses on visual search in large databases, and human behaviour understanding, particularly for video action recognition.
                  </p> 
              </div>
            </div>

          <!-- Cordelia Schmid -->            
          <div class="row">
            <div class="4u"> 
                <a href="http://thoth.inrialpes.fr/~schmid/" class="image image-centered"><img height=230 width=120 src="images/speakers/cordeliaschmid.jpg" alt="" /></a>
                <strong>Cordelia Schmid (INRIA)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    <b>Title:</b> Learning to Segment Moving Objects
                    <br><br>
                    <b>Abstract:</b> This talk addresses the task of segmenting moving objects in unconstrained videos. We introduce a novel two-stream neural network with an explicit memory module to achieve this. The two streams of the network encode spatial and temporal features in a video sequence respectively, while the memory module captures the evolution of objects over time. The module to build a “visual memory” in video, i.e., a joint representation of all the video frames, is realized with a convolutional recurrent unit learned from a small number of training video sequences. Given video frames as input, our approach first assigns each pixel an object or background label obtained with an encoder-decoder network that takes as input optical flow and is trained on synthetic data. Next, a “visual memory” specific to the video is acquired automatically without any manually-annotated frames. The visual memory is implemented with convolutional gated recurrent units, which allows to propagate spatial information over time. We evaluate our method extensively on two benchmarks, DAVIS and Freiburg-Berkeley motion segmentation datasets, and show state-of-the-art results.
                    <br><br>
                    <b>Bio:</b> Cordelia Schmid holds a M.S. degree in Computer Science from the University of Karlsruhe and a Doctorate, also in Computer Science, from the Institut National Polytechnique de Grenoble (INPG). Her doctoral thesis received the best thesis award from INPG in 1996. Dr. Schmid was a post-doctoral research assistant in the Robotics Research Group of Oxford University in 1996--1997. Since 1997 she has held a permanent research position at INRIA Grenoble Rhone-Alpes, where she is a research director and directs an INRIA team. Dr. Schmid is the author of over a hundred technical publications. She has been an Associate Editor for IEEE PAMI (2001--2005) and for IJCV (2004--2012), editor-in-chief for IJCV (2013---), a program chair of IEEE CVPR 2005 and ECCV 2012 as well as a general chair of IEEE CVPR 2015 and ECCV 2020. In 2006, 2014 and 2016, she was awarded the Longuet-Higgins prize for fundamental contributions in computer vision that have withstood the test of time. She is a fellow of IEEE. She was awarded an ERC advanced grant in 2013, the Humbolt research award in 2015 and the Inria & French Academy of Science Grand Prix in 2016. She was elected to the German National Academy of Sciences, Leopoldina, in 2017.
                  </p> 
              </div>
            </div>          

        </div> <!-- Talks end --> 
        
        <div id="Panel" class="tabcontent">
          <h3 style="margin: 20px 0px 0px 0px;">Panel</h3>
          <p>Panelists will answer questions and discuss about increasing diversity in computer vision.</p>
          <p>Feel free to ask your anonymous questions <a href="https://docs.google.com/forms/d/1BkdGAhHXcJl0ApC8lDNkmpEY04S312nldCVC0Ej09jY/">here</a>.</p>

            <!-- Chieko Asakawa --> 
            <div class="row">
            <div class="4u"> 
                <a href="http://researcher.watson.ibm.com/researcher/view.php?person=jp-CHIE" class="image image-centered"><img height=230 width=120 src="images/speakers/chiekoasakawa.png" alt="" /></a>
                <strong>Chieko Asakawa (IBM Research/CMU)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                     Chieko Asakawa is a blind Japanese computer scientist, known for her work at IBM Research – Tokyo in accessibility. A Netscape browser plug-in which she developed, the IBM Home Page Reader, became the most widely used web-to-speech system available. She is the recipient of numerous industry and government awards. Asakawa was born with normal sight, but after she injured her optic nerve in a swimming accident at age 11, she began losing her sight, and by age 14 she was fully blind. She earned a bachelor's degree in English literature at Otemon Gakuin University in Osaka in 1982 and then began a two-year computer programming course for blind people using an Optacon to translate print to tactile sensation. Chieko joined IBM in 1985 after completing the computer science courses for the blind at Nippon Lighthouse. She received a B.A. degree in English literature from Otemon Gakuin University in 1982, and a Ph.D in Engineering from the University of Tokyo in 2004. She is a member of the Association for Computing Machinery (ACM), the Information Processing Society of Japan, and IBM Academy of Technology. She was inducted into the Women in Technology International (WITI) Hall of Fame in 2003, and both within and outside of IBM, she has been actively working to help women engineers pursue technical careers. Chieko was appointed to IBM Fellow in 2009, IBM's most prestigious technical honor. In 2013, the government of Japan awarded the Medal of Honor with Purple Ribbon to Chieko for her outstanding contributions to accessibility research, including the development of the voice browser for the visually impaired.
                  </p> 
              </div>
            </div>          

            <!-- Andrea Frome --> 
            <div class="row">
            <div class="4u"> 
                <a href="https://clarifai.com/" class="image image-centered"><img height=230 width=120 src="images/speakers/andreafrome.jpg" alt="" /></a>
                <strong>Andrea Frome (Clarifai)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                  Dr. Andrea Frome earned her Ph.D. in Computer Science and Machine Learning in Jitendra Malik’s lab at UC Berkeley in 2007. Since then, her work in computer vision and machine learning has included: leading the visual recognition team within Street View which is especially known for its work blurring faces and license plates;  as a member of the Google Brain team, developing DeViSE for combining visual recognition with word embeddings and applying an attention RNN to fine-grained classification; and work on systems for Hillary for America at campaign headquarters for identity resolution and automatically reading canvassing surveys to reduce data entry. In January 2017, she joined Clarifai as Director of Research. Her non-work pursuits include doing volunteer work for <a href="flippable.org">flippable.org</a>, studying flying trapeze, and learning Argentine Tango.
                  </p> 
              </div>
            </div>          

            <!-- Raia Hadsell  --> 
            <div class="row">
            <div class="4u"> 
                <a href="http://raiahadsell.com/index.html" class="image image-centered"><img height=230 width=120 src="images/speakers/raiahadsell.jpg" alt="" /></a>
                <strong>Raia Hadsell (DeepMind)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                  Raia Hadsell, a senior research scientist at DeepMind, has worked on deep learning and robotics problems for over 10 years. Her early research developed the notion of manifold learning using Siamese networks, which has been used extensively for invariant feature learning. After completing a PhD with Yann LeCun, which featured a self-supervised deep learning vision system for a mobile robot, her research continued at Carnegie Mellon’s Robotics Institute and SRI International, and in early 2014 she joined DeepMind in London to study artificial general intelligence. Her current research focuses on the challenge of continual learning for AI agents and robots. While deep RL algorithms are capable of attaining superhuman performance on single tasks, they often cannot transfer that performance to additional tasks, especially if experienced sequentially. She has proposed neural approaches such as policy distillation, progressive nets, and elastic weight consolidation to solve the problem of catastrophic forgetting for agents and robots. 
                  </p> 
              </div>
            </div>          


            <!-- Naila Murray --> 
            <div class="row">
              <div class="4u"> 
                  <a href="http://www.xrce.xerox.com/About-XRCE/People/Naila-Murray" class="image image-centered"><img height=230 width=120 src="images/speakers/nailamurray.jpg" alt="" /></a>
                  <strong>Naila Murray (Naver Labs/NLE)</strong>
              </div>    
              <div class="8u">         
              <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                  Naila Murray graduated with a PhD in computer science from the Unversitat Autònoma de Barcelona. She also holds a master’s degree in computer vision and artificial intelligence from the Unversitat Autònoma de Barcelona, and a bachelor’s degree in electrical engineering from Princeton University. Her work in computer vision has involved research into biologically-inspired deep models of visual attention; fine-grained visual recognition, including participation in the winning team of the FGComp 2013 competition; and  computational models for visual aesthetic analysis. Naila is a senior scientist and manager of the computer vision group at Xerox Research Centre Europe. Currently, her research focuses on visual search in large databases, and human behaviour understanding, particularly for video action recognition.
                  </p> 
              </div>
            </div>

          <!-- Cordelia Schmid -->            
          <div class="row">
            <div class="4u"> 
                <a href="http://thoth.inrialpes.fr/~schmid/" class="image image-centered"><img height=230 width=120 src="images/speakers/cordeliaschmid.jpg" alt="" /></a>
                <strong>Cordelia Schmid (INRIA)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    Cordelia Schmid holds a M.S. degree in Computer Science from the University of Karlsruhe and a Doctorate, also in Computer Science, from the Institut National Polytechnique de Grenoble (INPG). Her doctoral thesis received the best thesis award from INPG in 1996. Dr. Schmid was a post-doctoral research assistant in the Robotics Research Group of Oxford University in 1996--1997. Since 1997 she has held a permanent research position at INRIA Grenoble Rhone-Alpes, where she is a research director and directs an INRIA team. Dr. Schmid is the author of over a hundred technical publications. She has been an Associate Editor for IEEE PAMI (2001--2005) and for IJCV (2004--2012), editor-in-chief for IJCV (2013---), a program chair of IEEE CVPR 2005 and ECCV 2012 as well as a general chair of IEEE CVPR 2015 and ECCV 2020. In 2006, 2014 and 2016, she was awarded the Longuet-Higgins prize for fundamental contributions in computer vision that have withstood the test of time. She is a fellow of IEEE. She was awarded an ERC advanced grant in 2013, the Humbolt research award in 2015 and the Inria & French Academy of Science Grand Prix in 2016. She was elected to the German National Academy of Sciences, Leopoldina, in 2017.
                  </p> 
              </div>
            </div>          

            <!-- Helge Seetzen  --> 
            <div class="row">
            <div class="4u"> 
                <a href="http://www.tandemlaunch.com" class="image image-centered"><img height=230 width=120 src="images/speakers/helgeseetzen.png" alt="" /></a>
                <strong>Helge Seetzen (TandemLaunch)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                 Helge is an award-winning technologist, entrepreneur, and a recognized global authority on technology transfer and display technologies. As General Partner of TandemLaunch, he works with inventors and entrepreneurs to build high growth technology companies. His past successes include the transformation of raw university IP into fully commercialized LED TV technology, including selling his last company - Brightside Technologies - to Dolby Laboratories after sealing partnerships with several of the largest consumer electronics companies in the world. Helge holds over 80 patents in the fields of display, camera and video technology.
                  </p> 
              </div>
            </div>          
        </div> <!-- Panel end --> 

        <div id="Orals" class="tabcontent">
          <h3 style="margin: 20px 0px 0px 0px;">Oral Presentations</h3>
          <p>A few accepted abstracts are invited to give oral presentations.</p>

          <p style="text-align:left"><b>Presenter instructions: </b>The presentations should be 12 minute talk and 3 minutes Q/A.</p>   
          <div class="divider">
              <hr class="left"/><b>Accepted orals</b><hr class="right" />
          </div>
          <div class="row" style="text-align:left"></div>
          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u"> <b>Presenter Name</b> </div><div class="2u">  <b>Institution</b> </div><div class="7u">  <b>Paper Title</b> </div></div>        
          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  Georgia Gkioxari  </div><div class="2u">  Facebook AI Research  </div><div class="7u">  Towards Better Instance-level Recognition </div></div>
          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  Lanlan Liu  </div><div class="2u">  University of Michigan  </div><div class="7u"> <a href="https://arxiv.org/pdf/1701.00299.pdf"> Dynamic Deep Neural Networks: Optimizing Accuracy-Efficiency Trade-offs by Selective Execution </a> </div></div>
          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  <a href="http://www.nasimsouly.com/">Nasim Souly</a> </div><div class="2u">  University of Central Florida </div><div class="7u">  <a href="https://arxiv.org/abs/1703.09695">Semi and Weakly Supervised Semantic Segmentation Using Generative Adversarial Network </a></div></div>
          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  Nour Karessli </div><div class="2u">  EyeEm </div><div class="7u">  Gaze Embeddings for Zero-Shot Image Classification  </div></div>
        </div>


        <div id="Posters" class="tabcontent">
          <h3 style="margin: 20px 0px 0px 0px;">Poster Presentations</h3>
          <p>Authors of all accepted abstracts (with or without travel grant) will present their work in a poster session.</p>                

          <p style="text-align:left"><b>Presenter instructions: </b>All posters should be installed in at most 10 minutes at the start of the poster session. The physical dimensions of the poster stands are similar to the main conference which is 8 feet wide by 4 feet high. Poster presenters can optionally use the <a href="http://cvpr2017.thecvf.com/files/cvpr17_poster_template_n.pptx">CVPR17 poster template</a> for more details on how to prepare their posters..</p>   
          <div class="divider">
              <hr class="left"/><b>Accepted abstracts</b><hr class="right" />
          </div><br>

<div class="row" style="text-align:left"><div class="1u"> <b>No</b> </div><div class="2u">  <b>Presenter Name</b> </div><div class="2u">  <b>Institution</b> </div><div class="7u">  <b>Paper Title</b> </div></div>        
<div class="row" style="text-align:left"><div class="1u"> 1 </div><div class="2u">  Aishwarya Agrawal </div><div class="2u">  Virginia Tech </div><div class="7u">  C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 2 </div><div class="2u">  <a href="https://imatge.upc.edu/web/people/amaia-salvador">Amaia Salvador </a> </div><div class="2u">  Universitat Politècnica de Catalunya  </div><div class="7u">  <a href="http://im2recipe.csail.mit.edu">Learning Cross-modal Embeddings for Cooking Recipes and Food Images</a> </div></div>
<div class="row" style="text-align:left"><div class="1u"> 3 </div><div class="2u">  <a href="
https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/people/anna-khoreva/">Anna Khoreva</a>  </div><div class="2u">  Max Planck Institute for Informatics  </div><div class="7u"> <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/weakly-supervised-learning/learning-video-object-segmentation-from-static-images/"> Learning Video Object Segmentation from Static Images </a></div></div>
<div class="row" style="text-align:left"><div class="1u"> 4 </div><div class="2u">  <a href="
https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/people/anna-khoreva/"> Anna Khoreva</a>  </div><div class="2u">  Max Planck Institute for Informatics  </div><div class="7u">  <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/weakly-supervised-learning/simple-does-it-weakly-supervised-instance-and-semantic-segmentation/"> Simple Does It: Weakly Supervised Instance and Semantic Segmentation </a> </div></div>
<div class="row" style="text-align:left"><div class="1u"> 5 </div><div class="2u">  Atreyee Sinha </div><div class="2u">  Edgewood College  </div><div class="7u">  Pre-trained CNNs for Artistic Style Recognition </div></div>
<div class="row" style="text-align:left"><div class="1u"> 6 </div><div class="2u">  <a href="http://www.igp.ethz.ch/en/personen/person-detail.html?persid=213159">Audrey Richard </a> </div><div class="2u">  ETH Zurich  </div><div class="7u">  <a href="http://www.prs.igp.ethz.ch/research/current_projects/large-scale_reconstruction.html">Efficient Semantic 3D Urban Modelling</a> </div></div>
<div class="row" style="text-align:left"><div class="1u"> 7 </div><div class="2u">  Boyi Li </div><div class="2u">  Huazhong University of Science and Technology </div><div class="7u">  <a href="http://boyilics.simplesite.com/435389823">End-to-end Dehazing Neural Network</a>  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 8  </div><div class="2u">  <a href="http://anakena.dcc.uchile.cl/~calvarez/"> Camila Alvarez</a>  </div><div class="2u">  Orand SA  </div><div class="7u">  Automatic clothing labeling of outdoor images </div></div>
<div class="row" style="text-align:left"><div class="1u"> 9  </div><div class="2u">  Chia-Yin Tsai </div><div class="2u">  Carnegie Mellon Unversity </div><div class="7u">  The Geometry of First-Returning Photons for Non-Line-of-Sight Imaging </div></div>
<div class="row" style="text-align:left"><div class="1u"> 10  </div><div class="2u">  <a href="http://homes.cs.washington.edu/~deepalia/">Deepali Aneja</a> </div><div class="2u">  University of Washington  </div><div class="7u"> <a href="http://grail.cs.washington.edu/projects/deepexpr/"> Learning Stylized Character Expressions from Humans </a></div></div>
<div class="row" style="text-align:left"><div class="1u"> 11  </div><div class="2u">  <a href="https://www.linkedin.com/in/divyaarc">Divyaa Ravichandran</a> </div><div class="2u">  GumGum Inc  </div><div class="7u">  Movies through the decades  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 12  </div><div class="2u">  Faezeh Amjadi </div><div class="2u">  Montreal University </div><div class="7u">  <a href="http://ieeexplore.ieee.org/document/7785141/">Comparison of Radial and Tangential Geometries for Cylindrical Panorama </a></div></div>
<div class="row" style="text-align:left"><div class="1u"> 13  </div><div class="2u">  Faezeh Tafazzoli  </div><div class="2u">  University of Louisville  </div><div class="7u">  Vehicle Make and Model Recognition for Automated Vehicular Surveillance </div></div>
<div class="row" style="text-align:left"><div class="1u"> 14  </div><div class="2u">  Fereshteh Sadeghi </div><div class="2u">  University of Washington  </div><div class="7u">  Collision Avoidance via Deep RL: Real Single-Image Flight without a Single Real Image </div></div>
<div class="row" style="text-align:left"><div class="1u"> 15  </div><div class="2u">  Georgia Gkioxari  </div><div class="2u">  Facebook AI Research  </div><div class="7u">  Towards Better Instance-level Recognition </div></div>
<div class="row" style="text-align:left"><div class="1u"> 16  </div><div class="2u">  Harini Kannan </div><div class="2u">  MIT </div><div class="7u">  GazeCascade: A Convolutional Neural Network for more efficient and accurate Eye-Tracking  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 17  </div><div class="2u">  <a href="https://www.linkedin.com/in/helenlouisebear">Helen Bear </a> </div><div class="2u">  University of East London </div><div class="7u">  <a href="https://www.researchgate.net/publication/317545178_Understanding_the_visual_speech_signal">Understanding the visual speech signal </a> </div></div>
<div class="row" style="text-align:left"><div class="1u"> 18  </div><div class="2u">  Hiranmayi Ranganathan </div><div class="2u">  Arizona State University  </div><div class="7u">  Deep Active Learning Framework for Image Classification </div></div>
<div class="row" style="text-align:left"><div class="1u"> 19  </div><div class="2u">  Hiteshi Jain  </div><div class="2u">  IIT Jodhpur </div><div class="7u">  <a href="http://home.iitj.ac.in/~jain.4/wicv_hiteshi.pdf">A Framework to Assess Sun Salutation</a>  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 20  </div><div class="2u">  Hsiao-Yu Tung </div><div class="2u">  Carnegie Mellon University  </div><div class="7u">  Adversarial Inversion: Self-supervision with Adversarial Priors </div></div>
<div class="row" style="text-align:left"><div class="1u"> 21  </div><div class="2u">  Huda Alamri </div><div class="2u">  Georgia Institute of Technology </div><div class="7u">  Hierarchical Tree-based Prior for Place Recognition </div></div>
<div class="row" style="text-align:left"><div class="1u"> 22  </div><div class="2u">  <a href="http://cs-people.bu.edu/hxu/">Huijuan Xu </a> </div><div class="2u">  Boston University </div><div class="7u"> <a href="https://arxiv.org/abs/1703.07814"> R-C3D: Region Convolutional 3D Network for Temporal Activity Detection </a> </div></div>
<div class="row" style="text-align:left"><div class="1u"> 23  </div><div class="2u">  Hyo Jin Kim </div><div class="2u">  University of North Carolina at Chapel Hill </div><div class="7u">  Learned Contextual Feature Reweighting for Image Geo-Localization </div></div>
<div class="row" style="text-align:left"><div class="1u"> 24  </div><div class="2u">  <a href="https://research.fb.com/people/demir-ilke/">Ilke Demir</a>  </div><div class="2u">  Purdue University / Facebook </div><div class="7u">  <a href="https://www.researchgate.net/publication/317551875_On_Generalized_Proceduralization_Approaches_for_Urban_Data">On Generalized Proceduralization Approaches for Urban Data </a> </div></div>
<div class="row" style="text-align:left"><div class="1u"> 25  </div><div class="2u">  Indrani Bhattacharya  </div><div class="2u">  Rensselaer Polytechnic Institute  </div><div class="7u">  A Cognitive Boardroom Without Cameras </div></div>
<div class="row" style="text-align:left"><div class="1u"> 26  </div><div class="2u">  Jadisha Ramirez Cornejo </div><div class="2u">  University of Campinas  </div><div class="7u">  Automatic Down Syndrome Detection Based on Facial Features Using a Geometric Descriptor </div></div>
<div class="row" style="text-align:left"><div class="1u"> 27  </div><div class="2u">  Jadisha Ramirez Cornejo </div><div class="2u">  University of Campinas  </div><div class="7u">  Robust Emotion Recognition of Occluded Facial Expressions </div></div>
<div class="row" style="text-align:left"><div class="1u"> 28  </div><div class="2u">  Jane Hung </div><div class="2u">  Broad Institute </div><div class="7u">  Applying Faster R-CNN for Object Detection on Malaria Images  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 29  </div><div class="2u">  Jiun-Yu Kao </div><div class="2u">  University of Southern California </div><div class="7u">  Tools for Analysis of Human Activity Based on Graph Signal Processing </div></div>
<div class="row" style="text-align:left"><div class="1u"> 30  </div><div class="2u">  Julia Peyre </div><div class="2u">  INRIA </div><div class="7u">  Weakly-supervised learning of visual relations  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 31  </div><div class="2u">  Jyoti Nigam </div><div class="2u">  IIT Mandi </div><div class="7u">  EgoTracker: Human Tracking with Re-identification in Egocentric Videos  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 32  </div><div class="2u">  Kaori Abe </div><div class="2u">  National Institute of Advanced Industrial Science and Technology  </div><div class="7u">  Weighted Feature Integration for Person Re-identification </div></div>
<div class="row" style="text-align:left"><div class="1u"> 33  </div><div class="2u">  Karla Brkic </div><div class="2u">  University of Zagreb  </div><div class="7u">  I Know That Person: Generative Full Body and Face De-Identification of People in Images </div></div>
<div class="row" style="text-align:left"><div class="1u"> 34  </div><div class="2u">  <a href="https://homes.cs.washington.edu/~kianae/">Kiana Ehsani</a>  </div><div class="2u">  University of Washington  </div><div class="7u">  SeGAN: Segmenting and Generating the Invisible  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 35  </div><div class="2u">  Kuan-Ting Chen  </div><div class="2u">  National Taiwan University  </div><div class="7u">  When Fashion Meets Big Data: Discriminative Mining of Best Selling Clothing Features  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 36  </div><div class="2u">  Lanlan Liu  </div><div class="2u">  University of Michigan  </div><div class="7u">  <a href="https://arxiv.org/pdf/1701.00299.pdf">Dynamic Deep Neural Networks: Optimizing Accuracy-Efficiency Trade-offs by Selective Execution </a> </div></div>
<div class="row" style="text-align:left"><div class="1u"> 37  </div><div class="2u">  Lea Schönherr </div><div class="2u">  Ruhr-Universität Bochum </div><div class="7u">  Spoofing Detection via Simultaneous Verification of Audio-Visual Synchronicity and Transcription  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 38  </div><div class="2u">  <a href="https://www.linkedin.com/in/linjie-li-purdue/">Li Linjie</a> </div><div class="2u">  Purdue University </div><div class="7u">  Modeling social perception of faces with deep learning  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 39  </div><div class="2u">  Lili Meng </div><div class="2u">  University of British Columbia  </div><div class="7u">  Backtracking Regression Forests for Accurate Camera Relocalization  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 40  </div><div class="2u">  Lisa Yan  </div><div class="2u">  Stanford University </div><div class="7u">  Deep Grade: A visual approach to grading student programming assignments  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 41  </div><div class="2u">  Luisa Zintgraf  </div><div class="2u">  University of Amsterdam </div><div class="7u">  Visualizing Deep Neural Network Decisions: Prediction Difference Analysis </div></div>
<div class="row" style="text-align:left"><div class="1u"> 42  </div><div class="2u">  <a href="https://www.linkedin.com/in/mariacamilaalvareztrivino/">Maria Camila Alvarez Triviño</a>  </div><div class="2u">  Universidad Autónoma de Occidente </div><div class="7u">  <a href="https://drive.google.com/file/d/0B0xB5mbT-AiyY2lSc0lBOTdTMGM/view?usp=sharing">Machine Learning on Retina Images for Diagnostic Decision Support</a> </div></div>
<div class="row" style="text-align:left"><div class="1u"> 43  </div><div class="2u">  <a href="https://www.linkedin.com/in/melodyguan/">Melody Guan</a> </div><div class="2u">  Google Brain  </div><div class="7u">  <a href="https://arxiv.org/pdf/1703.08774.pdf">Who Said What: Modeling individual labelers improves classification </a></div></div>
<div class="row" style="text-align:left"><div class="1u"> 44  </div><div class="2u">  Michela Paganini  </div><div class="2u">  Yale University </div><div class="7u">  Learning Particle Physics by Example: Location-Aware Generative Adversarial Networks for Physics Synthesis  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 45  </div><div class="2u">  Mijung Kim  </div><div class="2u">  Ghent University  </div><div class="7u">  Towards Using Few-shot Learning for Early Glaucoma Diagnosis in Small-Sized Datasets of High-Resolution Images  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 46  </div><div class="2u">  Miroslava Slavcheva </div><div class="2u">  Technical University of Munich  </div><div class="7u">  KillingFusion: Non-rigid 3D Reconstruction without Correspondences  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 47  </div><div class="2u">  Nai Chen Chang  </div><div class="2u">  Carnegie Mellon University  </div><div class="7u">  Low-shot Fine-grained Augmentation with Generative Adversarial Networks </div></div>
<div class="row" style="text-align:left"><div class="1u"> 48  </div><div class="2u">  <a href="http://www.nasimsouly.com/">Nasim Souly</a> </div><div class="2u">  University of Central Florida </div><div class="7u">  <a href="https://arxiv.org/abs/1703.09695">Semi and Weakly Supervised Semantic Segmentation Using Generative Adversarial Network</a> </div></div>
<div class="row" style="text-align:left"><div class="1u"> 49  </div><div class="2u">  Nour Karessli </div><div class="2u">  EyeEm </div><div class="7u">  Gaze Embeddings for Zero-Shot Image Classification  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 50  </div><div class="2u">  Obioma Pelka  </div><div class="2u">  University of Applied Sciences and Arts Dortmund  </div><div class="7u">  Automated Classification of Enhanced Radiographs with Deep Convolutional Neural Network </div></div>
<div class="row" style="text-align:left"><div class="1u"> 51  </div><div class="2u">  Prabhjot Kaur </div><div class="2u">  Indian Institute of Technology  </div><div class="7u">  Significance of Magnetic Resonance Image Details in Sparse Representation based Super Resolution  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 52	 </div><div class="2u">	 Priya Goyal	</div><div class="2u">	Facebook Inc.	</div><div class="7u">	Gated Cross Entropy Loss for Dense Object Detection	</div></div>																							
<div class="row" style="text-align:left"><div class="1u"> 53  </div><div class="2u">  Qing He </div><div class="2u">  Samsung SDS </div><div class="7u">  Intent Identification via Action Recognition and Long-term Sequence Association Learning  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 54  </div><div class="2u">  Qiu Yue </div><div class="2u">  University of Tsukuba </div><div class="7u">  Sensing and recognition of typical indoor family scenes using an RGB-D camera </div></div>
<div class="row" style="text-align:left"><div class="1u"> 55  </div><div class="2u">  Ruth Fong </div><div class="2u">  University of Oxford  </div><div class="7u">  Interpretable Explanations of Black Boxes by Meaningful Perturbation  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 56  </div><div class="2u">  <a href="https://samriddhijain.github.io/">Samriddhi Jain </a> </div><div class="2u">  IIT Mandi </div><div class="7u">  Object Triggered Egocentric Video Summarization </div></div>
<div class="row" style="text-align:left"><div class="1u"> 57  </div><div class="2u">  Shan Su </div><div class="2u">  University of Pennsylvania  </div><div class="7u">  Predicting Behaviors of Basketball Players from First Person Videos </div></div>
<div class="row" style="text-align:left"><div class="1u"> 58  </div><div class="2u">  Sherin Mathews  </div><div class="2u">  University of Delaware  </div><div class="7u">  Maximum Correntropy based Dictionary Learning framework for physical activity recognition using wearable sensors  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 59  </div><div class="2u">  Si Chen </div><div class="2u">  Georgia Institute of Technology </div><div class="7u">  Deep Auditory Hallucinations: Multi-Modal Music Generation from Actions </div></div>
<div class="row" style="text-align:left"><div class="1u"> 60  </div><div class="2u">  <a href="http://www.icst.pku.edu.cn/struct/people/Sijie%20Song.htm">Sijie Song </a> </div><div class="2u">  Peking University </div><div class="7u">  An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data </div></div>
<div class="row" style="text-align:left"><div class="1u"> 61  </div><div class="2u">  Sima Behpour  </div><div class="2u">  University of Illinois  </div><div class="7u">  Deep Adversarial Object Localization  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 62  </div><div class="2u">  <a href="http://spandanagella.com">Spandana Gella</a>  </div><div class="2u">  University of Edinburgh </div><div class="7u">  <a href="http://spandanagella.com/files/linguistic-action-recognition.pdf">A Linguistic viewpoint of Action Recognition in still Images </a> </div></div>
<div class="row" style="text-align:left"><div class="1u"> 63  </div><div class="2u">  Srishti Gautam  </div><div class="2u">  IIT Mandi </div><div class="7u">  Unsupervised Segmentation of Cervical Cancer Nuclei via Adaptive Clustering </div></div>
<div class="row" style="text-align:left"><div class="1u"> 64  </div><div class="2u">  Srishti Gautam  </div><div class="2u">  IIT Mandi </div><div class="7u">  CNN based Segmentation of Nuclei in PAP-Smear Images with Selective Pre-processing  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 65  </div><div class="2u">  <a href="http://acsweb.ucsd.edu/~stripath/">Subarna Tripathi</a>  </div><div class="2u">  UC San Diego  </div><div class="7u"><a href="https://vision.cornell.edu/se3/wp-content/uploads/2016/12/statistical-approach-continuous.pdf
https://arxiv.org/pdf/1612.06919.pdf">  Continuous Self-Calibrating Eye Gaze Tracking for Virtual Reality Systems </a></div></div>
<div class="row" style="text-align:left"><div class="1u"> 66  </div><div class="2u">  <a href="http://www.cs.utexas.edu/~vsub/">Subhashini Venugopalan</a>  </div><div class="2u">  University of Texas at Austin </div><div class="7u"> <a href="https://arxiv.org/abs/1606.07770"> Captioning Images with Diverse Objects</a>  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 67  </div><div class="2u">  Sudipta Banerjee  </div><div class="2u">  Michigan State University </div><div class="7u">  Generating an Image Phylogeny Tree for Near-Duplicate Iris Images </div></div>
<div class="row" style="text-align:left"><div class="1u"> 68  </div><div class="2u">  Thao Phung  </div><div class="2u">  University of Wyoming </div><div class="7u">  Learning to solve symbolic math from visual inputs  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 69  </div><div class="2u">  Tiffany Liu </div><div class="2u">  Stanford University </div><div class="7u">  Medical image analysis to identify subgroups of patients for personalized treatment </div></div>
<div class="row" style="text-align:left"><div class="1u"> 70  </div><div class="2u">  <a href="http://www.unaizahsan.com"> Unaiza Ahsan </a> </div><div class="2u">  Georgia Institute of Technology </div><div class="7u">  DiscrimNet: Semi-Supervised Action Recognition from Videos using Generative Adversarial Networks  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 71  </div><div class="2u">  <a href="https://utabuechler.github.io/">Uta Büchler</a> </div><div class="2u">  Heidelberg University </div><div class="7u">  <a href="https://utabuechler.github.io/papers/WiCV17.pdf">LSTM Self-Supervision for Detailed Behavior Analysis</a>  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 72  </div><div class="2u">  Vibha Gupta </div><div class="2u">  IIT Mandi </div><div class="7u">  An Integrated Multi-scale Model for Breast Cancer Histopathological Image Classification with Joint Colour-texture Features </div></div>
<div class="row" style="text-align:left"><div class="1u"> 73  </div><div class="2u">  Wenqian Liu  </div><div class="2u">  Northeastern University </div><div class="7u">  Multi-camera Multi-object Tracking  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 74  </div><div class="2u">  Yingxuan Zhu  </div><div class="2u">  Huawei R&amp;D USA  </div><div class="7u">  Design of an Integrated Emotion Recognition System  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 75  </div><div class="2u">  Yingxuan Zhu  </div><div class="2u">  Huawei R&amp;D USA  </div><div class="7u">  Composable Object Recognition and Reconstruction with Human in the Loop </div></div>
<div class="row" style="text-align:left"><div class="1u"> 76  </div><div class="2u">  Yu Wang </div><div class="2u">  University of Cambridge </div><div class="7u">  Sparse Bayesian Multi-Task Learning for Subspace Segmentation </div></div>
        </div>

        <div id="Dinner" class="tabcontent">
          <h3 style="margin: 20px 0px 0px 0px;">Mentoring Dinner on July 25</h3>
          <p style="font-style:italic;">by invitation only</p>
          <p>6:00 - 8:00 pm   Dinner sponsored by NVIDIA</p>
          <p align="left";>The dinner event is an opportunity to meet other female computer vision researchers. Poster presenters will be matched with senior computer vision researchers to share experience and career advice. Invitees will receive an e-mail and be asked to confirm attendance.</p>
          <p align="left";><b>*Note that the dinner takes place the evening before the main workshop day.*</b></p>
         
          <div class="divider">
              <hr class="left"/><b>Dinner speakers</b><hr class="right" />
          </div><br>

            <!-- Andrea Frome --> 
            <div class="row">
            <div class="4u"> 
                <a href="https://clarifai.com/" class="image image-centered"><img height=230 width=120 src="images/speakers/andreafrome.jpg" alt="" /></a>
                <strong>Andrea Frome (Clarifai)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                  Dr. Andrea Frome earned her Ph.D. in Computer Science and Machine Learning in Jitendra Malik’s lab at UC Berkeley in 2007. Since then, her work in computer vision and machine learning has included: leading the visual recognition team within Street View which is especially known for its work blurring faces and license plates;  as a member of the Google Brain team, developing DeViSE for combining visual recognition with word embeddings and applying an attention RNN to fine-grained classification; and work on systems for Hillary for America at campaign headquarters for identity resolution and automatically reading canvassing surveys to reduce data entry. In January 2017, she joined Clarifai as Director of Research. Her non-work pursuits include doing volunteer work for <a href="flippable.org">flippable.org</a>, studying flying trapeze, and learning Argentine Tango.
                  </p> 
              </div>
            </div>          
         
            <!-- OLGA RUSSAKOVSKY --> 
            <div class="row">
            <div class="4u"> 
                <a href="http://www.cs.princeton.edu/~olgarus/index.html" class="image image-centered"><img height=230 width=120 src="images/speakers/olgarussakovsky.jpg" alt="" /></a>
                <strong>Olga Russakovsky (Princeton University)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                  Olga Russakovsky is an Assistant Professor of Computer Science at Princeton University. She completed her PhD in Computer Science at Stanford University in August 2015 and her postdoc at the Robotics Institute of Carnegie Mellon University in June 2017. Her research is in computer vision, closely integrated with machine learning and human-computer interaction. Her work was featured in the New York Times and MIT Technology Review. She served as a Senior Program Committee member for WACV’16 (and CVPR’18 soon), led the ImageNet Large Scale Visual Recognition Challenge effort for two years, was the Publicity and Press chair at CVPR’16, and organized multiple workshops and tutorials on large-scale recognition at premier computer vision conferences ICCV’13, ECCV’14, CVPR’15, ICCV’15, CVPR’16, ECCV’16 and CVPR’17. In addition, she was the co-founder and director of the Stanford AI Laboratory’s outreach camp SAILORS (featured in Wired and published in SIGCSE’16) which educates high school girls about AI, and is the co-founder and board member of the AI4ALL foundation dedicated to cultivating a diverse group of future AI leaders. 
                  </p> 
              </div>
            </div>  
         
         </div>

        <script>
           function openCity(evt, cityName) {
               var i, tabcontent, tablinks;
               tabcontent = document.getElementsByClassName("tabcontent");
               for (i = 0; i < tabcontent.length; i++) {
                   tabcontent[i].style.display = "none";
               }
               tablinks = document.getElementsByClassName("tablinks");
               for (i = 0; i < tablinks.length; i++) {
                   tablinks[i].className = tablinks[i].className.replace(" active", "");
               }
               document.getElementById(cityName).style.display = "block";
               evt.currentTarget.className += " active";
           }
           
           // Get the element with id="defaultOpen" and click on it
           document.getElementById("defaultOpen").click();
        </script>

          <br><br><br><br><br> 
        </div>
        <!-- Footer Wrapper -->
        <div class="footer">
            <div class="container">
                    <div class="row">
                            <div class="7u">
             <p>&copy; WiCV 2017</p>
            </div>
            <div class="1u">
                   <a href="mailto:wicv17-organizers@googlegroups.com" class="fa fa-envelope" style="font-size:1.25em;" data-placement="top" data-toggle="tooltip" title="Email"></a>
                       </div>
                       <div class="1u">                       
                                                        <a href="https://twitter.com/wicvworkshop" class="fa fa-twitter" data-placement="top" style="font-size:1.25em;"  data-toggle="tooltip" title="Twitter"></a>
                                                     </div>
                                                         <div class="1u">
                                                               <a href="https://www.facebook.com/WomenInComputerVision/" class="fa fa-facebook" data-placement="top" style="font-size:1.25em;" data-toggle="tooltip" title="Facebook"></a>
                                                               </div>
                                                           </div>
                                                           </div>
                                                         </div>
        <script src="//static.getclicky.com/js" type="text/javascript"></script>
        <script type="text/javascript">try{ clicky.init(100926441); }catch(e){}</script>
        <noscript><p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/100926441ns.gif" /></p></noscript>
    </body>
</html>
