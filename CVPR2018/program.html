<!DOCTYPE HTML>


<!--
 Berkeley Vision and Learning Center (BVLC)
 
 Design based on:
 Strongly Typed 1.0 by HTML5 UP
 html5up.net | @n33co
 Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
 -->
<html>
    <head>
        <title>WiCV</title>
        <meta http-equiv="content-type" content="text/html; charset=utf-8" />
        <meta name="description" content="" />
        <meta name="keywords" content="" />
        <meta name="viewport" content="width=1040" />
        <link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600|Arvo:700" rel="stylesheet" type="text/css" />
        <!--[if lte IE 8]><script src="js/html5shiv.js"></script><![endif]-->
        <script src="js/jquery.min.js"></script>
        <script src="js/jquery.dropotron.js"></script>
        <script src="js/jquery.dotdotdot.min.js"></script>
        <script src="js/config.js"></script>
        <script src="js/skel.min.js"></script>
        <script src="js/skel-panels.min.js"></script>
        <script src="js/jquery.slides.min.js"></script>
        <link rel="stylesheet" href="css/font-awesome.min.css">
            <noscript>
                <link rel="stylesheet" href="css/style.css" />
                <link rel="stylesheet" href="css/style-desktop.css" />
                <link rel="stylesheet" href="css/skel-noscript.css" />
            </noscript>
        
        <style>
        
        /* Style the tab */
        div.tab {
            margin: auto;
            width: 75%;
            padding: 0px 12px; 
            overflow: hidden;
            border: 1px solid #ccc;
            background-color: #f1f1f1;
        }
        
        /* Style the buttons inside the tab */
        div.tab button {
            background-color: inherit;
            float: left;
            border: none;
            outline: none;
            cursor: pointer;
            padding: 14px 16px;
            transition: 0.3s;
            font-size: 17px;
        }
        
        /* Change background color of buttons on hover */
        div.tab button:hover {
            background-color: #ddd;
        }
        
        /* Create an active/current tablink class */
        div.tab button.active {
            background-color: #ccc;
        }
        
        /* Style the tab content */
        .tabcontent {
            margin: auto;
            width: 75%;
            display: none;
            padding: 6px 12px;
            border: 1px solid #ccc;
            border-top: none;
        }
        </style>

            </head>
    <body class="homepage">
        <!-- Header Wrapper -->
        <div id="header-wrapper">
            <!-- Header -->
            <div id="header" class="container">
                <!-- Logo -->
                <!--<h1 id="logo">
                 <a id="home" href="#"></a>
                 </h1>-->
                <!-- Nav -->
                <nav id="nav">
                    <ul style="padding-top: 10px; padding-bottom: 2em">
                        <li>
                            <a href="index.html#header" class="">
                                <span style="
                                    position: relative;
                                    width: 261px;
                                    display: inline-block;
                                    height: 10px;
                                    ">
                                    <img src="images/wicv_logo_simple.png" style="
                                    position: absolute;
                                    left: 0;
                                    width: 111px;
                                    top: -20px;
                                    "/>
                                </span>
                            </a>
                        </li>
                        
                        <li>
                            <a href="index.html"><span>Home</span></a>
                        </li>
                        <li>
                            <a href="program.html"><span>Program</span></a>
                        </li>
                        <li>
                            <a href="faq.html"><span>FAQ</span></a>
                        </li>
                        <li>
                            <a href="committee.html"><span>Committee</span></a>
                        </li>
                        <li>
                            <a href="participation.html"><span>Call for Participation</span></a>
                        </li>
                        <li>
                            <a href="contact.html"><span>Contact</span></a>
                            
                        </li>
                        <li>
                           <a href="#"><span>WiCV</span></a>
                           <ul>
                              <li><a href="https://wicvworkshop.github.io/ECCV2018/index.html">WiCV @ECCV 2018</a></li>
                              <li><a href="https://wicvworkshop.github.io/CVPR2018/index.html">WiCV @CVPR 2018</a></li>
                              <li><a href="https://wicvworkshop.github.io/2017/index.html">WiCV 2017</a></li>
                              <li><a href="https://sites.google.com/site/wicv2016/home">WiCV 2016</a></li>
                              <li><a href="https://sites.google.com/site/wicv2015/home">WiCV 2015</a></li>
                           </ul>
                        </li>
                    </ul>
                </nav>
            </div>
        </div>
        
        <div class="banner-wrapper">
            <div class="inner">
                <!-- Banner -->
                <section class="banner container">
                    <br>
                    <h2 id="faculty">Program</h2>
                    
                </section>
            </div>
        </div>
        <div class="features-wrapper">
                  
        <p style="color:red;"><strong>* The final schedule will be announced here. * </strong></p>

        <div class="tab">
          <button class="tablinks" onclick="openCity(event, 'Schedule')" id="defaultOpen">Schedule</button>
          <button class="tablinks" onclick="openCity(event, 'Talks')">Talks</button>
          <button class="tablinks" onclick="openCity(event, 'Panel')">Panel</button>
          <button class="tablinks" onclick="openCity(event, 'Orals')">Orals</button>
          <button class="tablinks" onclick="openCity(event, 'Posters')">Posters</button>
          <button class="tablinks" onclick="openCity(event, 'Dinner')">Dinner</button>
        </div>
        
        <div id="Schedule" class="tabcontent">
          <h3 style="margin: 20px 0px 0px 0px;">Main Workshop on June 22</h3>
          <!--<h5 style="color:red;font-style:italic; margin: 0px 0px 20px 0px;">tentative schedule</h5> -->
          <h5 style="color:red; margin: 0px 0px 20px 0px;">Room: 251 - D</h5>
           
          <p style="margin: 0px 0px 20px 40px;text-align:left;">
                8:50  - 9:00  &emsp;&emsp;&emsp;<b>Introduction</b> <br>
                9:00  - 9:30  &emsp;&emsp;&emsp;<b>Invited Talk1 (Jessica Hodgins, Carnegie Mellon University)</b><br>                
                9:30  - 10:00 &ensp;&emsp;&emsp;<b>Invited Talk2 (Laura Leal Taixe, Technical University Munich)</b><br>
                10:00 - 11:30 &emsp;&emsp;<b>Poster Session and Morning Break</b><br>
                11:30 - 11:50  &emsp;&emsp;<b> Oral Session1</b><br>
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Learnable PINs: Cross-Modal Embeddings for Person Identity, by Arsha Nagrani (Oxford University)</a><br>
                11:50 - 12:10  &emsp;&emsp;<b> Oral Session2 </b><br>
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;ARC: Adversarial Robust Cuts for Semi-Supervised and Multi-Label Classification, by Sima
Behpour (University of Illinois) </a><br>
                12:10 - 13:30  &emsp;&emsp;<b> Lunch </b><br>
                13:30 - 14:00  &emsp;&emsp;<b> Invited Talk3 (Octavia Camps, Northeastern University)</b><br>
                14:00 - 14:20  &emsp;&emsp;<b> Oral Session3 </b><br>
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Don’t Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering, by Aishwarya Agrawal (Georgia Tech) </a><br>
                14:20 - 14:50  &emsp;&emsp;<b> Oral Session4 </b><br>
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;On the iterative refinement of densely connected representation levels for semantic segmentation, by Arantxa Casanova (MILA) </a><br>
                14:50 - 15:20  &emsp;&emsp;<b> Invited Talk4 (Carol E. Reiley, drive.ai)</b><br>
                15:20 - 15:40  &emsp;&emsp;<b> Oral Session5 </b><br>
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Gradient-free policy architecture search and adaptation, by Sayna Ebrahimi (UC Berkeley) </a><br>
                15:40 - 16:00  &emsp;&emsp;<b> Oral Session6 </b><br>
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Joint Event Detection and Description in Continuous Video Streams, by Huijuan Xu (Boston University) </a><br>
                16:00 - 16:30  &emsp;&emsp;<b> Afternoon Break </b><br>
                16:30 - 17:10  &emsp;&emsp;<b> Panel </b><br>
                17:10 - 17:20  &emsp;&emsp;<b>Closing Remarks</b> <br>
          </p>

        </div>
        
        <div id="Talks" class="tabcontent">
          <h2 style="margin: 20px 0px 0px 0px;">Keynote Talks</h2>
          <p>Keynote speakers will give technical talks about their research in computer vision.</p> 

        <!-- Octavia Camps --> 
          <div class="row">
            <div class="4u"> 
                <a href="http://robustsystems.ece.neu.edu" class="image image-centered"><img height=230 width=120 src="images/speakers/OctaviaCamps.png" alt="" /></a>
                <strong> Octavia I. Camps (Northeastern University)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                   <b>Title:</b> Dynamics-based Invariants for Video Understanding
                    <br><br>
                    <b>Abstract:</b> The power of geometric invariants to provide solutions to computer vision problems has been recognized for a long time. On the other hand, dynamics-based invariants are often overlooked. Yet, visual data come in streams: videos are temporal sequences of frames, images are ordered sequences of rows of pixels and contours are chained sequences of edges. In this talk, I will discuss the key role that systems theory can play in timely extracting and exploiting dynamics-based invariants to capture actionable information that is very sparsely encoded in high dimensional data streams. The central theme of this approach is the use of dynamical models, and their associated invariants, as an information-encoding paradigm. We will show that embedding problems in the conceptual world of dynamical systems makes available a rich, extremely powerful resource base, leading to robust solutions, or, in cases where the underlying problem is intrinsically hard, to computationally tractable approximations with sub optimality certificates.  We will illustrate these ideas in the context of several practical applications: crowd-sourcing video, activity recognition, human re-identification and video prediction.

                    <br><br>
                    <b>Bio:</b> Octavia Camps received a B.S. degree in computer science and a B.S. degree in electrical engineering from the Universidad de la Republica (Uruguay), and a M.S. and a Ph.D. degree in electrical engineering from the University of Washington. Since 2006, she is a Professor in the Electrical and Computer Engineering Department at Northeastern University. From 1991 to 2006 she was a faculty of Electrical Engineering and of Computer Science and Engineering at The Pennsylvania State University. Prof. Camps was a visiting researcher at the Computer Science Department at Boston University during Spring 2013 and in 2000, she was a visiting faculty at the California Institute of Technology and at the University of Southern California.  She is an associate editor of Computer Vision and Image Understanding (CVIU). Her main research interests include robust computer vision, image processing, and machine learning.
                  </p> 
              </div>
            </div> 

          <!-- Jessica K. Hodgins -->  
         
          <div class="row">
            <div class="4u"> 
                <a href="http://www.cs.cmu.edu/~jkh/" class="image image-centered"><img height=230 width=120 src="images/speakers/jessicaHodings.jpg" alt="" /></a>
                <strong> Jessica K. Hodgins (Carnegie Mellon University / Facebook AI Research)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                   <b>Title:</b> Capture and Animation of Human Motion
                    <br><br>
                    <b>Abstract:</b> In this talk, Jessica Hodgins will present research on constructing controllers for human motion from first principles, on capturing data of human motion and conclude with recent results on using captured data to learn controllers. Throughout the talk, she will reflect on the lessons learned, both from the research itself and how the projects came to be.

                    <br><br>
                    <b>Bio:</b> Jessica Hodgins is a Professor in the Robotics Institute and Computer Science Department at Carnegie Mellon University. From 2008-2016, she founded and ran research labs for Disney, rising to VP of Research and leading the labs in Pittsburgh and Los Angeles. From 2005-2015, she was Associate Director for Faculty in the Robotics Institute, running the promotion and tenure process and creating a mentoring program for pre-tenure faculty. Prior to moving to Carnegie Mellon in 2000, she was an Associate Professor and Assistant Dean in the College of Computing at Georgia Institute of Technology. She received her Ph.D. in Computer Science from Carnegie Mellon University in 1989. Her research focuses on computer graphics, animation, and robotics with an emphasis on generating and analyzing human motion. She has received a NSF Young Investigator Award, a Packard Fellowship, and a Sloan Fellowship. She was editor-in-chief of ACM Transactions on Graphics from 2000-2002 and ACM SIGGRAPH Papers Chair in 2003. She was an elected director at large on the ACM SIGGRAPH Executive Committee from 2012-2017 and in 2017 she was elected ACM SIGGRAPH President. In 2010, she was awarded the ACM SIGGRAPH Computer Graphics Achievement Award and in 2017 she was awarded the Steven Anson Coons Award for Outstanding Creative Contributions to Computer Graphics.
                  </p> 
              </div>
            </div>   
 
          <!-- Laura Leal-Taixé --> 
          <div class="row">
            <div class="4u"> 
                <a class="image image-centered"><img height=230 width=120 src="images/speakers/LauraLeal.png" alt="" /></a>
                <strong> Laura Leal-Taixé (Technical University of Munich)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                   <b>Title:</b> CNN vs SIFT-based localization: out with the old?
                    <br><br>
                    <b>Abstract:</b> Recently Deep Learning has achieved such a great performance in so many vision tasks that researcher are starting to use it to solve any imaginable task, disregarding previous methods optimised over decades of research work.
Image-based localization is one of those problems: a classic task in Computer Vision where given an image, we are asked to find its camera pose and orientation with respect to a given model which can have a scale ranging from a city to a small room.
Recently researchers have turned into CNNs for pose estimation disregarding previous literature and even disregarding basic epipolar geometry.
In our first work, we put both methods to test: on the one hand, we clearly show CNN-based localization has still an incredibly long way to go with respect to SIFT-based methods; on the other hand, we present an indoor dataset in which classic methods suffer due to textureless surfaces and repetitive structures.
Afterwards, I will also present our first attempt towards a fully scalable method based on relative pose estimation that allows us to localize a camera in any given scene with a single network (and even use epipolar geometry!).
I will show that a marriage between CNN and Computer Vision classic knowledge is still possible and very much desirable in today's research landscape.

                    <br><br>
                    <b>Bio:</b> Prof. Laura Leal-Taixé is leading the Dynamic Vision and Learning group at the Technical University of Munich, Germany. 
She received her Bachelor and Master degrees in Telecommunications Engineering from the Technical University of Catalonia (UPC), Barcelona. She did her Master Thesis at Northeastern University, Boston, USA and received her PhD degree (Dr.-Ing.) from the Leibniz University Hannover, Germany. 
During her PhD she did a one-year visit at the Vision Lab at the University of Michigan, USA. 
She also spent two years as a postdoc at the Institute of Geodesy and Photogrammetry of ETH Zurich, Switzerland and one year at the Technical University of Munich.
Her research interests are dynamic scene understanding, in particular multiple object tracking and segmentation, as well as machine learning for video analysis.
                  </p> 
              </div>
            </div>       

            <!-- Carol E. Reiley -->  
         
          <div class="row">
            <div class="4u"> 
                <a href="https://creiley.wordpress.com/" class="image image-centered"><img height=230 width=120 src="images/speakers/CarolEReiley.png" alt="" /></a>
                <strong> Carol E. Reiley ( drive.ai)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
 <b>Title:</b> When bias in AI and product design means life or death.
                    <br><br>
                    <b>Abstract:</b> Description: A talk about bias in AI products could impact humanity - the state of where we are and the largest pitfalls all throughout the process. Will go through a history of AI products and discuss where bias creeps in and how to create a safer, more delightful products for the world. Since we're at the start of the AI revolution, this lays groundwork for how to be thoughtful as a company and as a consumer. 
                    <br><br>
                  <b>Bio: </b>  
                 Carol E. Reiley has been at the forefront of robotics and AI for over 15 years and was the youngest member on the IEEE Robotics and Automation board. She's worked on highly regulated products in a variety of different applications such as space, underwater and medical.  She did her graduate work at Johns Hopkins University as an NSF Fellow researching how humans and robots interact, worked on the da Vinci system at Intuitive Surgical, and most recently cofounded and was the President of a self-driving car startup drive.ai. She's raised over $77M and built a partnership with Lyft, Grab, and automotive companies. Reiley was selected by Forbes as one of the twenty incredible women advancing artificial intelligence research. She currently holds over six patents, over a dozen publications, and was the first female engineer on the cover of MAKE magazine. She’s been featured in major publications such as MIT Tech Review, NYTimes, Harper’s Bazaar and given several Tedx Talks. She is also a published author and is now starting a new startup.
                  </p> 
              </div>
            </div>   

      

              

          <!-- Cordelia Schmid -->  
<!--          
          <div class="row">
            <div class="4u"> 
                <a href="http://thoth.inrialpes.fr/~schmid/" class="image image-centered"><img height=230 width=120 src="images/speakers/cordeliaschmid.jpg" alt="" /></a>
                <strong>Cordelia Schmid (INRIA)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    <b>Title:</b> Learning to Segment Moving Objects
                    <br><br>
                    <b>Abstract:</b> This talk addresses the task of segmenting moving objects in unconstrained videos. We introduce a novel two-stream neural network with an explicit memory module to achieve this. The two streams of the network encode spatial and temporal features in a video sequence respectively, while the memory module captures the evolution of objects over time. The module to build a “visual memory” in video, i.e., a joint representation of all the video frames, is realized with a convolutional recurrent unit learned from a small number of training video sequences. Given video frames as input, our approach first assigns each pixel an object or background label obtained with an encoder-decoder network that takes as input optical flow and is trained on synthetic data. Next, a “visual memory” specific to the video is acquired automatically without any manually-annotated frames. The visual memory is implemented with convolutional gated recurrent units, which allows to propagate spatial information over time. We evaluate our method extensively on two benchmarks, DAVIS and Freiburg-Berkeley motion segmentation datasets, and show state-of-the-art results.
                    <br><br>
                    <b>Bio:</b> Cordelia Schmid holds a M.S. degree in Computer Science from the University of Karlsruhe and a Doctorate, also in Computer Science, from the Institut National Polytechnique de Grenoble (INPG). Her doctoral thesis received the best thesis award from INPG in 1996. Dr. Schmid was a post-doctoral research assistant in the Robotics Research Group of Oxford University in 1996--1997. Since 1997 she has held a permanent research position at INRIA Grenoble Rhone-Alpes, where she is a research director and directs an INRIA team. Dr. Schmid is the author of over a hundred technical publications. She has been an Associate Editor for IEEE PAMI (2001--2005) and for IJCV (2004--2012), editor-in-chief for IJCV (2013---), a program chair of IEEE CVPR 2005 and ECCV 2012 as well as a general chair of IEEE CVPR 2015 and ECCV 2020. In 2006, 2014 and 2016, she was awarded the Longuet-Higgins prize for fundamental contributions in computer vision that have withstood the test of time. She is a fellow of IEEE. She was awarded an ERC advanced grant in 2013, the Humbolt research award in 2015 and the Inria & French Academy of Science Grand Prix in 2016. She was elected to the German National Academy of Sciences, Leopoldina, in 2017.
                  </p> 
              </div>
            </div>          
-->
        </div> <!-- Talks end --> 
        
        <div id="Panel" class="tabcontent">
          <h3 style="margin: 20px 0px 0px 0px;">Panel</h3>
          <p>Panelists will answer questions and discuss about increasing diversity in computer vision.</p>
          <p>Feel free to ask your anonymous questions <a href="https://docs.google.com/forms/d/1aQh398qJmXV0jDea8sM10TRDEqYYVaV-tlhoCAkZCik/viewform?edit_requested=true">here</a>.</p> 

<!-- Michael Black --> 
          <div class="row">
            <div class="4u"> 
                <a  href="https://ps.is.tuebingen.mpg.de/person/black" class="image image-centered"><img height=230 width=120 src="images/speakers/MichaelBlack.jpg" alt="" /></a>
                <strong> Michael Black (Max Planck / Amazon)</strong>
              </div>      
                <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">    
                    <b>Bio:</b> Michael Black received his B.Sc. from the University of British Columbia (1985), his M.S. from Stanford (1989), and his Ph.D. from Yale University (1992). After post-doctoral research at the University of Toronto, he worked at Xerox PARC as a member of research staff and area manager. From 2000 to 2010 he was on the faculty of Brown University in the Department of Computer Science (Assoc. Prof. 2000-2004, Prof. 2004-2010). He is one of the founding directors at the Max Planck Institute for Intelligent Systems in Tübingen, Germany, where he leads the Perceiving Systems department and serves as Managing Director. He is also a Distinguished Amazon Scholar, an Honorarprofessor at the University of Tuebingen, and Adjunct Professor at Brown University. His work has won several awards including the IEEE Computer Society Outstanding Paper Award (1991), Honorable Mention for the Marr Prize (1999 and 2005), the 2010 Koenderink Prize for Fundamental Contributions in Computer Vision, and the 2013 Helmholtz Prize for work that has stood the test of time. He is a foreign member of the Royal Swedish Academy of Sciences. In 2013 he co-founded Body Labs Inc., which was acquired by Amazon in 2017.
                  </p> 
              </div>
            </div>


<!-- Octavia Camps --> 
          <div class="row">
            <div class="4u"> 
                <a  href="http://robustsystems.ece.neu.edu" class="image image-centered"><img height=230 width=120 src="images/speakers/OctaviaCamps.png" alt="" /></a>
                <strong> Octavia I. Camps (Northeastern University)</strong>
              </div>      
                <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">    
                    <b>Bio:</b> Octavia Camps received a B.S. degree in computer science and a B.S. degree in electrical engineering from the Universidad de la Republica (Uruguay), and a M.S. and a Ph.D. degree in electrical engineering from the University of Washington. Since 2006, she is a Professor in the Electrical and Computer Engineering Department at Northeastern University. From 1991 to 2006 she was a faculty of Electrical Engineering and of Computer Science and Engineering at The Pennsylvania State University. Prof. Camps was a visiting researcher at the Computer Science Department at Boston University during Spring 2013 and in 2000, she was a visiting faculty at the California Institute of Technology and at the University of Southern California.  She is an associate editor of Computer Vision and Image Understanding (CVIU). Her main research interests include robust computer vision, image processing, and machine learning.
                  </p> 
              </div>
            </div> 
     

 <!-- Dima Damen -->
            <div class="row">
            <div class="4u"> 
                <a href="https://dimadamen.github.io/" class="image image-centered"><img height=230 width=120 src="images/speakers/DimaDamen.jpg" alt="" /></a>
                <strong>Dima Damen (University of Bristol)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                 <b>Bio:</b> Dima Damen is a Senior Lecturer (Associate Professor) in Computer Vision at the University of Bristol, United Kingdom. Received her PhD from the University of Leeds (2009). Dima's research interests are in the automatic understanding of object interactions, actions and activities using static and wearable visual (and depth) sensors. Dima co-chaired BMVC 2013, is area chair for BMVC (2014-2018), associate editor of Pattern Recognition (2017-). She was selected as a Nokia Research collaborator in 2016, and as an Outstanding Reviewer in ICCV17, CVPR13 and CVPR12. She currently supervises 9 PhD students, and 3 postdoctoral researchers. 
                  </p> 
              </div>
            </div>


<!-- Jessica K. Hodgins -->  
         
          <div class="row">
            <div class="4u"> 
                <a href="http://www.cs.cmu.edu/~jkh/" class="image image-centered"><img height=230 width=120 src="images/speakers/jessicaHodings.jpg" alt="" /></a>
                <strong> Jessica K. Hodgins (Carnegie Mellon University / Facebook AI Research)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    <b>Bio:</b> Jessica Hodgins is a Professor in the Robotics Institute and Computer Science Department at Carnegie Mellon University. From 2008-2016, she founded and ran research labs for Disney, rising to VP of Research and leading the labs in Pittsburgh and Los Angeles. From 2005-2015, she was Associate Director for Faculty in the Robotics Institute, running the promotion and tenure process and creating a mentoring program for pre-tenure faculty. Prior to moving to Carnegie Mellon in 2000, she was an Associate Professor and Assistant Dean in the College of Computing at Georgia Institute of Technology. She received her Ph.D. in Computer Science from Carnegie Mellon University in 1989. Her research focuses on computer graphics, animation, and robotics with an emphasis on generating and analyzing human motion. She has received a NSF Young Investigator Award, a Packard Fellowship, and a Sloan Fellowship. She was editor-in-chief of ACM Transactions on Graphics from 2000-2002 and ACM SIGGRAPH Papers Chair in 2003. She was an elected director at large on the ACM SIGGRAPH Executive Committee from 2012-2017 and in 2017 she was elected ACM SIGGRAPH President. In 2010, she was awarded the ACM SIGGRAPH Computer Graphics Achievement Award and in 2017 she was awarded the Steven Anson Coons Award for Outstanding Creative Contributions to Computer Graphics.
                  </p> 
              </div>
            </div>   
 

 <!-- Laura Leal-Taixé -->
          <div class="row">
            <div class="4u"> 
                <a class="image image-centered"><img height=230 width=120 src="images/speakers/LauraLeal.png" alt="" /></a>
                <strong> Laura Leal-Taixé (Technical University of Munich)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    <b>Bio:</b> Prof. Laura Leal-Taixé is leading the Dynamic Vision and Learning group at the Technical University of Munich, Germany. 
She received her Bachelor and Master degrees in Telecommunications Engineering from the Technical University of Catalonia (UPC), Barcelona. She did her Master Thesis at Northeastern University, Boston, USA and received her PhD degree (Dr.-Ing.) from the Leibniz University Hannover, Germany. 
During her PhD she did a one-year visit at the Vision Lab at the University of Michigan, USA. 
She also spent two years as a postdoc at the Institute of Geodesy and Photogrammetry of ETH Zurich, Switzerland and one year at the Technical University of Munich.
Her research interests are dynamic scene understanding, in particular multiple object tracking and segmentation, as well as machine learning for video analysis.
                  </p> 
              </div>
            </div> 

        <!-- Xin Lu -->
            <div class="row">
            <div class="4u"> 
                <a class="image image-centered"><img height=230 width=120 src="images/speakers/XinLu.jpg" alt="" /></a>
                <strong> Xin Lu (Adobe)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                 <b>Bio:</b> Xin Lu is researcher, developer, and manager at Adobe. She has been working on deep learning and its application on computer vision and image processing. Her recent research interests include deep neural network architecture optimization, neural network pruning, and image generation. Her research work has been deployed across Adobe desktop and mobile products.  
Prior to joining Adobe, Xin received her Ph.D. from The Pennsylvania State University. Her thesis focused on image aesthetics assessment, emotion recognition and image denoising.  
                  </p> 
              </div>
            </div>
 

            <!-- Jitendra Malik  -->  
            <div class="row">
            <div class="4u"> 
                <a href="https://people.eecs.berkeley.edu/~malik/" class="image image-centered"><img height=230 width=120 src="images/speakers/JitendraMalik.jpg" alt="" /></a>
                <strong>Jitendra Malik (UC Berkeley / Facebook)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                 <b>Bio:</b> Jitendra Malik is Arthur J. Chick Professor of EECS at UC Berkeley, and a Director of Research and Site Lead at Facebook AI Research in Menlo Park. He has published widely in computer vision, computational modeling of human vision, and machine learning.  Several well-known concepts and algorithms arose in this research, such as anisotropic diffusion, normalized cuts, high dynamic range imaging, shape contexts and R-CNN.  Jitendra received the Distinguished Researcher in Computer Vision Award from IEEE, the K.S. Fu Prize from IAPR, and the Allen Newell award from ACM  and AAAI.  He has been elected to the National Academy of Sciences, the National Academy of Engineering and the American Academy of Arts and Sciences. 
                  </p> 
              </div>
            </div> 

 <!-- Carol E. Reiley -->
 <div class="row">
            <div class="4u"> 
                <a href="https://creiley.wordpress.com/" class="image image-centered"><img height=230 width=120 src="images/speakers/CarolEReiley.png" alt="" /></a>
                <strong> Carol E. Reiley ( drive.ai)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                 <b>Bio:</b> Carol E. Reiley has been at the forefront of robotics and AI for over 15 years and was the youngest member on the IEEE Robotics and Automation board. She's worked on highly regulated products in a variety of different applications such as space, underwater and medical.  She did her graduate work at Johns Hopkins University as an NSF Fellow researching how humans and robots interact, worked on the da Vinci system at Intuitive Surgical, and most recently cofounded and was the President of a self-driving car startup drive.ai. She's raised over $77M and built a partnership with Lyft, Grab, and automotive companies. Reiley was selected by Forbes as one of the twenty incredible women advancing artificial intelligence research. She currently holds over six patents, over a dozen publications, and was the first female engineer on the cover of MAKE magazine. She’s been featured in major publications such as MIT Tech Review, NYTimes, Harper’s Bazaar and given several Tedx Talks. She is also a published author and is now starting a new startup.
             </p> 
        </div>
            </div>


            <!-- Helge Seetzen  --> 
<!-- 
            <div class="row">
            <div class="4u"> 
                <a href="http://www.tandemlaunch.com" class="image image-centered"><img height=230 width=120 src="images/speakers/helgeseetzen.png" alt="" /></a>
                <strong>Helge Seetzen (TandemLaunch)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                 Helge is an award-winning technologist, entrepreneur, and a recognized global authority on technology transfer and display technologies. As General Partner of TandemLaunch, he works with inventors and entrepreneurs to build high growth technology companies. His past successes include the transformation of raw university IP into fully commercialized LED TV technology, including selling his last company - Brightside Technologies - to Dolby Laboratories after sealing partnerships with several of the largest consumer electronics companies in the world. Helge holds over 80 patents in the fields of display, camera and video technology.
                  </p> 
              </div>
            </div> 
-->         
        </div> <!-- Panel end --> 

        <div id="Orals" class="tabcontent">
          <h3 style="margin: 20px 0px 0px 0px;">Oral Presentations</h3>
          <p>A few accepted papers are invited to give oral presentations.</p>

          <p style="text-align:left"><b>Presenter instructions: </b> The presentations should be 15 minute talk and 5 minutes Q/A.</p>   
          <div class="divider">
              <hr class="left"/><b>Accepted orals</b><hr class="right" />
          </div>
          <div class="row" style="text-align:left"></div>
          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u"> <b>Presenter Name</b> </div><div class="2u">  <b>Institution</b> </div><div class="7u">  <b>Paper Title</b> </div></div>  
      
          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  Arantxa Casanova </div><div class="2u">  MILA  </div><div class="7u">  On the iterative refinement of densely connected representation levels for semantic segmentation 
</div></div>

          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">    </div><div class="2u">    </div><div class="7u">     <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">    
                    <b>Abstract:</b> State-of-the-art semantic segmentation approaches increase the receptive field of their models by using either a downsampling path composed of poolings/strided convolutions or successive dilated convolutions. However, it is not clear which operation leads to best results. In this paper, we systematically study the differences introduced by distinct receptive field enlargement methods and their impact on the performance of a novel architecture, called Fully Convolutional DenseResNet (FC-DRN). FC-DRN has a densely connected backbone composed of residual networks. Following standard image segmentation architectures, receptive field enlargement operations that change the representation level are interleaved among residual networks. This allows the model to exploit the benefits of both residual and dense connectivity patterns, namely: gradient flow, iterative refinement of representations, multi-scale feature combination and deep supervision. In order to highlight the potential of our model, we test it on the challenging CamVid urban scene understanding benchmark and make the following observations: 1) downsampling operations outperform dilations when the model is trained from scratch, 2) dilations are useful during the finetuning step of the model, 3) coarser representations require less refinement steps, and 4) ResNets (by model construction) are good regularizers, since they can reduce the model capacity when needed. Finally, we compare our architecture to alternative methods and report state-of-the-art result on the Camvid dataset, with at least twice fewer parameters. 
                  </p> 
</div></div>

             




          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  Huijuan Xu  </div><div class="2u">  University of Boston  </div><div class="7u">  Joint Event Detection and Description in Continuous Video Streams  </div></div>

          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">    </div><div class="2u">    </div><div class="7u">     <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">    
                    <b>Abstract:</b> Dense video captioning is a fine-grained video understanding task that involves two sub-problems: localizing distinct events in a long video stream, and generating captions for the localized events. We propose the Joint Event Detection and Description Network (JEDDi-Net), which solves the dense video captioning task in an end-to-end fashion. Our model continuously encodes the input video stream with three-dimensional convolutional layers, proposes variable-length temporal events based on pooled features, and generates their captions. Unlike existing approaches, our event proposal generation and language captioning networks are trained jointly and end-to-end, allowing for improved temporal segmentation. In order to explicitly model temporal relationships between visual events and their captions in a single video, we also propose a two-level hierarchical captioning module that keeps track of context. On the large-scale ActivityNet Captions dataset, JEDDi-Net demonstrates improved results as measured by standard metrics. We also present the first dense captioning results on the TACoS-MultiLevel dataset. 
                  </p> 
</div></div>



          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  Sima Behpour </div><div class="2u">  University of Illinois </div><div class="7u">   ARC: Adversarial Robust Cuts for Semi-Supervised and Multi-Label Classification </div></div>

          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">    </div><div class="2u">    </div><div class="7u">     <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">    
                    <b>Abstract:</b> Many structured prediction tasks arising in computer vision and natural language processing tractably reduce to making minimum cost cuts in graphs with edge weights learned us-
ing maximum margin methods. Unfortunately, the hinge loss used to construct these methods often provides a particularly
loose bound on the loss function of interest (e.g., the Hamming loss). We develop Adversarial Robust Cuts (ARC), an approach  that  poses  the  learning  task  as  a  minimax  game between predictor and “label approximator” based on minimum  cost  graph  cuts.  Unlike  maximum  margin  methods, this game-theoretic perspective always provides meaningful bounds  on  the  Hamming  loss.  We  conduct  multi-label  and semi-supervised binary prediction experiments that demonstrate the benefits of our approach.
                  </p> 
</div></div>


          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  Arsha Nagrani </div><div class="2u">  University of Oxford </div><div class="7u">  Learnable PINs: Cross-Modal Embeddings for Person Identity  </div></div>


          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">    </div><div class="2u">    </div><div class="7u">     <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">    
                    <b>Abstract:</b> We propose and investigate an identity sensitive joint embedding of face and voice. Such an embedding enables cross-modal retrieval from voice to face and from face to voice. We make the following four contributions: first, we show that the embedding can be learnt from videos of talking faces, without requiring any identity labels, using a form of cross-modal self-supervision; second, we develop a curriculum learning schedule for hard negative mining targeted to this task, that is essential for learning to proceed successfully; third, we demonstrate and evaluate cross-modal retrieval for identities unseen and unheard during training over a number of scenarios and establish a benchmark for this novel task; finally, we show an application of using the joint embedding for automatically retrieving and labelling characters in TV dramas. 
                  </p> 
</div></div>


          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  Sayna Ebrahimi </div><div class="2u">  UC Berkeley </div><div class="7u">  Gradient-free policy architecture search and adaptation  </div></div>        


          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">    </div><div class="2u">    </div><div class="7u">     <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">    
                    <b>Abstract:</b> We develop a method for policy architecture search and adaptation via gradient-free optimization which can learn to perform autonomous driving tasks. By learning from both demonstration and environmental reward we develop a model that can learn with relatively few early catastrophic failures. We first learn an architecture of appropriate complexity to perceive aspects of world state relevant to the expert demonstration, and then mitigate the effect of domain-shift during deployment by adapting a policy demonstrated in a source domain to rewards obtained in a target environment. We show that our approach allows safer learning than baseline methods, offering a reduced cumulative crash metric over the agent's lifetime as it learns to drive in a realistic simulated environment. 
                  </p> 
</div></div>


          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  Aishwarya Agrawal </div><div class="2u">  Georgia Tech </div><div class="7u">  Don’t Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering  </div></div>


          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">    </div><div class="2u">    </div><div class="7u">     <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">    
                    <b>Abstract:</b> A number of studies have found that today's Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from 'cheating' by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model -- Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models. 
                  </p> 
</div></div>



        </div>


        <div id="Posters" class="tabcontent">
          <h3 style="margin: 20px 0px 0px 0px;">Poster Presentations</h3>
          <p>Authors of all accepted papers (with or without travel grant) will present their work in a poster session.</p>                

          <p style="text-align:left"><b>Presenter instructions: 
</b>All posters should be installed in at most 10 minutes at the start of the poster session at 10:00. The physical dimensions of the poster stands are 8 feet wide by 4 feet high. Poster presenters can optionally use the <a href="http://cvpr2018.thecvf.com/files/cvpr18_poster_template_n.pptx"> CVPR18 poster template</a> for more details on how to prepare their posters. You do not need to use this template, but please read the <a href="http://cvpr2018.thecvf.com/submission/presenter_instructions"> instructions </a> carefully and prepare your posters accordingly. Please note your poster number below to find your board.</p>  

          <div class="divider">
              <hr class="left"/><b>Accepted Posters</b><hr class="right" />
          </div><br>

<div class="row" style="text-align:left"><div class="1u"> No </div><div class="2u">  Presenter Name </div><div class="2u">  Institution </div><div class="7u">  Paper Title </div></div>
      <hr/> 
 <br>

<div class="row" style="text-align:left"><div class="1u"> 1 </div><div class="2u">  Doris Antensteiner </div><div class="2u">  Svorad Stolc Austrian Institute of Technology </div><div class="7u"> Variational Depth and Normal Fusion Algorithms for 3D Reconstruction  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 2 </div><div class="2u"> Mengjiao Wang </div><div class="2u"> Imperial College London   </div><div class="7u">  A Neuro-Tensorial Approach For Learning Disentangled Representations </div></div>
<div class="row" style="text-align:left"><div class="1u"> 3 </div><div class="2u"> Ksenia Bittner </div><div class="2u">   German Aerospace Center   </div><div class="7u">  Automatic Large-Scale 3D Building Shape Refinement Using Conditional Generative Adversarial Networks </div></div>
<div class="row" style="text-align:left"><div class="1u"> 4 </div><div class="2u">  Franziska Mueller </div><div class="2u">   MPI Informatics   </div><div class="7u">  GANerated Hands for Real-Time 3D Hand Tracking from Monocular RGB </div></div>
<div class="row" style="text-align:left"><div class="1u"> 5 </div><div class="2u"> Yanran Joyce Wang </div><div class="2u">  Northwestern University  </div><div class="7u">  Quick Adaption of Segmentation FCN via Network Modulation </div></div>
<div class="row" style="text-align:left"><div class="1u"> 6 </div><div class="2u"> Derya Akkaynak </div><div class="2u">  University of Haifa  </div><div class="7u">  A Revised Underwater Image Formation Model </div></div>
<div class="row" style="text-align:left"><div class="1u"> 7 </div><div class="2u"> Dena Bazazian </div><div class="2u">  CVC  </div><div class="7u">  Word Spotting in Scene Images based on Character Recognition </div></div>
<div class="row" style="text-align:left"><div class="1u"> 8 </div><div class="2u"> Tamar Rott Shaham </div><div class="2u">  Technion  </div><div class="7u">  Deformation Aware Image Compression </div></div>
<div class="row" style="text-align:left"><div class="1u"> 9 </div><div class="2u"> Meng Zheng </div><div class="2u">  Rensselaer Polytechnic Institute  </div><div class="7u">  RPIfield: A New Dataset for Temporally Evaluating Person Re-Identification </div></div>
<div class="row" style="text-align:left"><div class="1u"> 10 </div><div class="2u"> Ilke Demir </div><div class="2u">  Facebook  </div><div class="7u">  A Holistic Framework for Addressing the World using Machine Learning </div></div>
<div class="row" style="text-align:left"><div class="1u"> 11 </div><div class="2u"> Bojana Gajic </div><div class="2u">  Computer Vision Center  </div><div class="7u">  Cross-domain fashion image retrieval </div></div>
<div class="row" style="text-align:left"><div class="1u"> 12 </div><div class="2u"> Simone Meyer </div><div class="2u">  ETH Zurich  </div><div class="7u">  PhaseNet for Video Frame Interpolation </div></div>
<div class="row" style="text-align:left"><div class="1u"> 13 </div><div class="2u"> Jingya Liu </div><div class="2u">  City College of New York  </div><div class="7u">  Recognizing Elevator Buttons and Labels for Blind Navigation </div></div>
<!-- <div class="row" style="text-align:left"><div class="1u"> 14 </div><div class="2u"> Amanda Duarte </div><div class="2u">  Universitat Politecnica de Catalunya  </div><div class="7u">  Cross-modal Embeddings for Video and Audio Retrieval </div></div> -->
<div class="row" style="text-align:left"><div class="1u"> 14 </div><div class="2u">   Kuan-Ting Chen </div><div class="2u">  National Taiwan University  </div><div class="7u">   Netizen-Style Commenting on Fashion Photos: Dataset and Diversity Measures </div></div>
<div class="row" style="text-align:left"><div class="1u"> 15 </div><div class="2u"> Ruth Fong </div><div class="2u">  University of Oxford  </div><div class="7u">  Net2Vec: Explaining how Concepts are Encoded in Deep Neural Networks </div></div>
<div class="row" style="text-align:left"><div class="1u"> 16 </div><div class="2u"> Ozge Yalcinkaya </div><div class="2u">  Hacettepe University  </div><div class="7u">  I-ME: Iterative Model Evolution for Learning Activities From Weakly Labeled Videos
 </div></div>
<div class="row" style="text-align:left"><div class="1u"> 17 </div><div class="2u"> Kanami Yamagishi </div><div class="2u">  Waseda University  </div><div class="7u">  Cosmetic Features Extraction by a Single Image Makeup Decomposition </div></div>
<div class="row" style="text-align:left"><div class="1u"> 18 </div><div class="2u"> Rosaura Vidal Mata  </div><div class="2u">  University of Notre Dame  </div><div class="7u">  UG^2: a Video Benchmark for Assessing the Impact of Image Restoration and Enhancement on Automatic Visual Recognition </div></div>
<div class="row" style="text-align:left"><div class="1u"> 19 </div><div class="2u"> Avantika Singh </div><div class="2u">  IIT Mandi  </div><div class="7u">  Encapsulating the impact of transfer learning, domain knowledge and training strategies in deep-learning based architecture: A biometric based case study </div></div>
<div class="row" style="text-align:left"><div class="1u"> 20 </div><div class="2u"> Jadisha Ramirez Cornejo </div><div class="2u">  University of Campinas  </div><div class="7u">  Dynamic Facial Expression Recognition through Visual Rhythm and Motion History Image </div></div>
<div class="row" style="text-align:left"><div class="1u"> 21 </div><div class="2u"> Karla Brkic </div><div class="2u">  University of Zagreb  </div><div class="7u">  Keep it Short: Understanding Traffic Scenes From Very Short Representations </div></div>
<div class="row" style="text-align:left"><div class="1u"> 22 </div><div class="2u"> Hiya Roy </div><div class="2u">  University of Tokyo  </div><div class="7u">  Do hashtags help? - Image aesthetics prediction using only hashtags </div></div>
<div class="row" style="text-align:left"><div class="1u"> 23 </div><div class="2u"> Marcella Cornia </div><div class="2u">  University of Modena and Reggio Emilia  </div><div class="7u">  SAM: Pushing the Limits of Saliency Prediction Models </div></div>
<div class="row" style="text-align:left"><div class="1u"> 24 </div><div class="2u"> Yi Zhu </div><div class="2u">  University of Chinese Academy of Sciences  </div><div class="7u">  To Be Focused: Efficient Weakly Supervised Learning via Soft Proposal Network </div></div>
<div class="row" style="text-align:left"><div class="1u"> 25 </div><div class="2u"> Nezihe Merve Gürel </div><div class="2u">  ETH Zürich  </div><div class="7u">  Towards More Accurate Radio Telescope Images </div></div>
<!-- <div class="row" style="text-align:left"><div class="1u"> 27 </div><div class="2u"> Moi Hoon Yap </div><div class="2u">  MMU  </div><div class="7u">  Facial Wrinkles Annotator: Coarse and fine wrinkles </div></div> -->
<div class="row" style="text-align:left"><div class="1u"> 26 </div><div class="2u"> Yi Zhu </div><div class="2u">  University of Chinese Academy of Sciences  </div><div class="7u">  Learning Peak Response for Weakly Supervised Instance-level Segmentation </div></div>
<div class="row" style="text-align:left"><div class="1u"> 27 </div><div class="2u"> Mahdieh Poostchi </div><div class="2u">  NIH  </div><div class="7u">  
Multi-scale Spatially weighted Local Histogram in O(1) </div></div>
<div class="row" style="text-align:left"><div class="1u"> 28 </div><div class="2u"> Mahdieh Poostchi </div><div class="2u">  NIH  </div><div class="7u">  Malaria Parasite Detection and Quantification Using Deep Neural Network </div></div>
<div class="row" style="text-align:left"><div class="1u"> 29 </div><div class="2u"> Mikayla Timm </div><div class="2u">  University of Massachusetts Amherst  </div><div class="7u">  Large-Scale Ecological Analyses of Animals in the Wild using Computer Vision </div></div>
<div class="row" style="text-align:left"><div class="1u"> 30 </div><div class="2u"> Ekta Gujral </div><div class="2u">  University of California, Riverside   </div><div class="7u">  FMVR:Shall I Get My Video Back? Feature Matching based Video Reconstruction </div></div>
<div class="row" style="text-align:left"><div class="1u"> 31 </div><div class="2u"> Murium Iqbal </div><div class="2u">  Overstock  </div><div class="7u">  Discovering Style Trends through Deep Visually Aware Latent Product Embeddings </div></div>
<div class="row" style="text-align:left"><div class="1u"> 32 </div><div class="2u"> Anis Davoudi </div><div class="2u">  university of florida  </div><div class="7u">  
Autonomous detection of disruptions in the intensive care unit using deep mask R-CNN </div></div>
<div class="row" style="text-align:left"><div class="1u"> 33 </div><div class="2u"> Jing Zhang </div><div class="2u">  Australian National University  </div><div class="7u">  
Deep Saliency Detection: From Supervised Learning to Unsupervised Learning </div></div>
<div class="row" style="text-align:left"><div class="1u"> 34 </div><div class="2u"> Akram Bayat </div><div class="2u">  UMass Boston  </div><div class="7u">  
Multi-Resolution Deep Object Recognition Network </div></div>
<div class="row" style="text-align:left"><div class="1u"> 35 </div><div class="2u"> Fariba Zohrizadeh </div><div class="2u">  University of Texas at Arlington  </div><div class="7u">  Image Segmentation using Sparse Subset Selection </div></div>
<div class="row" style="text-align:left"><div class="1u"> 36 </div><div class="2u"> Ivona Tautkute </div><div class="2u">  Tooploox  </div><div class="7u">  
I Know How You Feel: Emotion Recognition with Facial Landmarks </div></div>
<div class="row" style="text-align:left"><div class="1u"> 37 </div><div class="2u"> Jyoti Islam </div><div class="2u">  Georgia State University  </div><div class="7u">  
Early Diagnosis of Alzheimer's Disease: A Neuroimaging Study with Deep Learning Architectures </div></div>
<div class="row" style="text-align:left"><div class="1u"> 38 </div><div class="2u"> Sheila Pinto Caceres </div><div class="2u">  University of Sydney  </div><div class="7u">  
Activity Recognition under Energy Constraints </div></div>
<div class="row" style="text-align:left"><div class="1u"> 39 </div><div class="2u"> vibha gupta </div><div class="2u">  IIT Mandi  </div><div class="7u">  
Hybridization of Feature Selection Methods Based on Tournament Design: HEp-2 Cell Image Classification </div></div>
<div class="row" style="text-align:left"><div class="1u"> 40 </div><div class="2u"> Reham Abobeah </div><div class="2u">  Egypt Japan University of Science and Technology  </div><div class="7u">  Wearable RGB Camera-based Navigation System for the Visually Impaired </div></div>
<div class="row" style="text-align:left"><div class="1u"> 41 </div><div class="2u"> Rajvi Shah </div><div class="2u">  IIIT Hyderabad  </div><div class="7u">  
View-graph Selection Framework for SfM </div></div>
<div class="row" style="text-align:left"><div class="1u"> 42 </div><div class="2u"> Arantxa Casanova </div><div class="2u">  MILA   </div><div class="7u">  
On the iterative refinement of densely connected representation levels for semantic segmentation  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 43 </div><div class="2u"> Huijuan Xu  </div><div class="2u">  University of Boston   </div><div class="7u">  
Joint Event Detection and Description in Continuous Video Streams   </div></div>
<div class="row" style="text-align:left"><div class="1u"> 44 </div><div class="2u"> Sima Behpour  </div><div class="2u">  University of Illinois   </div><div class="7u">  
ARC: Adversarial Robust Cuts for Semi-Supervised and Multi-Label Classification    </div></div>
<div class="row" style="text-align:left"><div class="1u"> 45 </div><div class="2u">Arsha Nagrani  </div><div class="2u"> University of Oxford   </div><div class="7u">  
Learnable PINs: Cross-Modal Embeddings for Person Identity    </div></div>
<div class="row" style="text-align:left"><div class="1u"> 46 </div><div class="2u">Sayna Ebrahimi   </div><div class="2u"> UC Berkeley    </div><div class="7u">  
Gradient-free policy architecture search and adaptation     </div></div>
<div class="row" style="text-align:left"><div class="1u"> 47 </div><div class="2u">Aishwarya Agrawal    </div><div class="2u"> Georgia Tech     </div><div class="7u">  
Don’t Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering    </div></div>


        </div>

        <div id="Dinner" class="tabcontent">
          <h3 style="margin: 20px 0px 0px 0px;">Mentoring Dinner on June 21 </h3>
          <!-- <p style="font-style:italic;">by invitation only</p> -->
          <p>6:30 - 10:00 pm   Dinner sponsored by  <strong> <font color="dodgerblue">Facebook</font></strong> </p> 
          <p align="left";>The dinner event is an opportunity to meet other female computer vision researchers. Poster presenters will be matched with senior computer vision researchers to share experience and career advice. Invitees will receive an e-mail and be asked to confirm attendance.</p>
          <p align="left";><b>*Note that the dinner takes place the evening before the main workshop day.*</b></p>  
         
          <div class="divider">
              <hr class="left"/><b>Dinner speakers</b><hr class="right" />
          </div><br>

        <!-- Timnit Gebru -->
            <div class="row">
            <div class="4u"> 
                <a href="https://www.microsoft.com/en-us/research/people/tigebru/" class="image image-centered"><img height=230 width=120 src="images/TimnitGebru.jpg
" alt="" /></a>
                <strong> Timnit Gebru (Microsoft Research)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                 <b>Bio:</b> Timnit Gebru works in the Fairness Accountability Transparency and Ethics (FATE) group at Microsoft Research, New York. Prior to joining Microsoft Research, she was a PhD student in the Stanford Artificial Intelligence Laboratory, studying computer vision under Fei-Fei Li. Her main research interest is in data mining large-scale, publicly available images to gain sociological insight, and working on computer vision problems that arise as a result, including fine-grained image recognition, scalable annotation of images, and domain adaptation. She is currently studying the ethical considerations underlying any data mining project, and methods of auditing and mitigating bias in sociotechnical systems. The New York Times, MIT Tech Review and others have recently covered her work. As a cofounder of the group Black in AI, she works to both increase diversity in the field and reduce the negative impacts of racial bias in training data used for human-centric machine learning models. 
                  </p> 
              </div>
            </div> 

        <!-- Dima Damen -->
            <div class="row">
            <div class="4u"> 
                <a href="https://dimadamen.github.io/" class="image image-centered"><img height=230 width=120 src="images/speakers/DimaDamen.jpg" alt="" /></a>
                <strong>Dima Damen (University of Bristol)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                 <b>Bio:</b> Dima Damen is a Senior Lecturer (Associate Professor) in Computer Vision at the University of Bristol, United Kingdom. Received her PhD from the University of Leeds (2009). Dima's research interests are in the automatic understanding of object interactions, actions and activities using static and wearable visual (and depth) sensors. Dima co-chaired BMVC 2013, is area chair for BMVC (2014-2018), associate editor of Pattern Recognition (2017-). She was selected as a Nokia Research collaborator in 2016, and as an Outstanding Reviewer in ICCV17, CVPR13 and CVPR12. She currently supervises 9 PhD students, and 3 postdoctoral researchers. 
                  </p> 
              </div>
            </div>


        <!-- Xin Lu -->
            <div class="row">
            <div class="4u"> 
                <a class="image image-centered"><img height=230 width=120 src="images/speakers/XinLu.jpg" alt="" /></a>
                <strong> Xin Lu (Adobe)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                 <b>Bio:</b> Xin Lu is researcher, developer, and manager at Adobe. She has been working on deep learning and its application on computer vision and image processing. Her recent research interests include deep neural network architecture optimization, neural network pruning, and image generation. Her research work has been deployed across Adobe desktop and mobile products.  
Prior to joining Adobe, Xin received her Ph.D. from The Pennsylvania State University. Her thesis focused on image aesthetics assessment, emotion recognition and image denoising.  
                  </p> 
              </div>
            </div>
 
            <!-- OLGA RUSSAKOVSKY --> 
<!--
            <div class="row">
            <div class="4u"> 
                <a href="http://www.cs.princeton.edu/~olgarus/index.html" class="image image-centered"><img height=230 width=120 src="images/speakers/olgarussakovsky.jpg" alt="" /></a>
                <strong>Olga Russakovsky (Princeton University)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                  Olga Russakovsky is an Assistant Professor of Computer Science at Princeton University. She completed her PhD in Computer Science at Stanford University in August 2015 and her postdoc at the Robotics Institute of Carnegie Mellon University in June 2017. Her research is in computer vision, closely integrated with machine learning and human-computer interaction. Her work was featured in the New York Times and MIT Technology Review. She served as a Senior Program Committee member for WACV’16 (and CVPR’18 soon), led the ImageNet Large Scale Visual Recognition Challenge effort for two years, was the Publicity and Press chair at CVPR’16, and organized multiple workshops and tutorials on large-scale recognition at premier computer vision conferences ICCV’13, ECCV’14, CVPR’15, ICCV’15, CVPR’16, ECCV’16 and CVPR’17. In addition, she was the co-founder and director of the Stanford AI Laboratory’s outreach camp SAILORS (featured in Wired and published in SIGCSE’16) which educates high school girls about AI, and is the co-founder and board member of the AI4ALL foundation dedicated to cultivating a diverse group of future AI leaders. 
                  </p> 
              </div>
            </div>  
-->

         
         </div>

        <script>
           function openCity(evt, cityName) {
               var i, tabcontent, tablinks;
               tabcontent = document.getElementsByClassName("tabcontent");
               for (i = 0; i < tabcontent.length; i++) {
                   tabcontent[i].style.display = "none";
               }
               tablinks = document.getElementsByClassName("tablinks");
               for (i = 0; i < tablinks.length; i++) {
                   tablinks[i].className = tablinks[i].className.replace(" active", "");
               }
               document.getElementById(cityName).style.display = "block";
               evt.currentTarget.className += " active";
           }
           
           // Get the element with id="defaultOpen" and click on it
           document.getElementById("defaultOpen").click();
        </script>

          <br><br><br><br><br> 
        </div>
        <!-- Footer Wrapper -->
        <div class="footer">
            <div class="container">
                    <div class="row">
                            <div class="7u">
             <p>&copy; WiCV 2018</p>
            </div>
            <div class="1u">
                   <a href="mailto:wicv18-organizers@googlegroups.com" class="fa fa-envelope" style="font-size:1.25em;" data-placement="top" data-toggle="tooltip" title="Email"></a>
                       </div>
                       <div class="1u">                       
                                                        <a href="https://twitter.com/wicvworkshop" class="fa fa-twitter" data-placement="top" style="font-size:1.25em;"  data-toggle="tooltip" title="Twitter"></a>
                                                     </div>
                                                         <div class="1u">
                                                               <a href="https://www.facebook.com/WomenInComputerVision/" class="fa fa-facebook" data-placement="top" style="font-size:1.25em;" data-toggle="tooltip" title="Facebook"></a>
                                                               </div>
                                                           </div>
                                                           </div>
                                                         </div>
        <script src="//static.getclicky.com/js" type="text/javascript"></script>
        <script type="text/javascript">try{ clicky.init(100926441); }catch(e){}</script>
        <noscript><p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/100926441ns.gif" /></p></noscript>
    </body>
</html>
