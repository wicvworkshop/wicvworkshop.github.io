
<!DOCTYPE HTML>


<!--
 Berkeley Vision and Learning Center (BVLC)
 
 Design based on:
 Strongly Typed 1.0 by HTML5 UP
 html5up.net | @n33co
 Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
 -->
<html>
    <head>
        <title>WiCV</title>
        <meta http-equiv="content-type" content="text/html; charset=utf-8" />
        <meta name="description" content="" />
        <meta name="keywords" content="" />
        <meta name="viewport" content="width=1040" />
        <link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600|Arvo:700" rel="stylesheet" type="text/css" />
        <!--[if lte IE 8]><script src="js/html5shiv.js"></script><![endif]-->
        <script src="js/jquery.min.js"></script>
        <script src="js/jquery.dropotron.js"></script>
        <script src="js/jquery.dotdotdot.min.js"></script>
        <script src="js/config.js"></script>
        <script src="js/skel.min.js"></script>
        <script src="js/skel-panels.min.js"></script>
        <script src="js/jquery.slides.min.js"></script>
        <link rel="stylesheet" href="css/font-awesome.min.css">
            <noscript>
                <link rel="stylesheet" href="css/style.css" />
                <link rel="stylesheet" href="css/style-desktop.css" />
                <link rel="stylesheet" href="css/skel-noscript.css" />
            </noscript>
        
        <style>
        
        /* Style the tab */
        div.tab {
            margin: auto;
            width: 75%;
            padding: 0px 12px; 
            overflow: hidden;
            border: 1px solid #ccc;
            background-color: #f1f1f1;
        }
        
        /* Style the buttons inside the tab */
        div.tab button {
            background-color: inherit;
            float: left;
            border: none;
            outline: none;
            cursor: pointer;
            padding: 14px 16px;
            transition: 0.3s;
            font-size: 17px;
        }
        
        /* Change background color of buttons on hover */
        div.tab button:hover {
            background-color: #ddd;
        }
        
        /* Create an active/current tablink class */
        div.tab button.active {
            background-color: #ccc;
        }
        
        /* Style the tab content */
        .tabcontent {
            margin: auto;
            width: 75%;
            display: none;
            padding: 6px 12px;
            border: 1px solid #ccc;
            border-top: none;
        }
        </style>

            </head>
    <body class="homepage">
        <!-- Header Wrapper -->
        <div id="header-wrapper">
            <!-- Header -->
            <div id="header" class="container">
                <!-- Logo -->
                <!--<h1 id="logo">
                 <a id="home" href="#"></a>
                 </h1>-->
                <!-- Nav -->
                <nav id="nav">
                    <ul style="padding-top: 10px; padding-bottom: 2em">
                        <li>
                            <a href="index.html#header" class="">
                                <span style="
                                    position: relative;
                                    width: 261px;
                                    display: inline-block;
                                    height: 10px;
                                    ">
                                    <img src="images/wicv_logo_simple.png" style="
                                    position: absolute;
                                    left: 0;
                                    width: 111px;
                                    top: -20px;
                                    "/>
                                </span>
                            </a>
                        </li>
                        
                        <li>
                            <a href="index.html"><span>Home</span></a>
                        </li>
                        <li>
                            <a href="program.html"><span>Program</span></a>
                        </li>
                        <li>
                            <a href="faq.html"><span>FAQ</span></a>
                        </li>
                        <li>
                            <a href="committee.html"><span>Committee</span></a>
                        </li>
                        <li>
                            <a href="participation.html"><span>Call for Participation</span></a>
                        </li>
                        <li>
                            <a href="contact.html"><span>Contact</span></a>
                            
                        </li>
                        <li>
                           <a href="#"><span>WiCV</span></a>
                           <ul>
                              <li><a href="https://wicvworkshop.github.io/ECCV2018/index.html">WiCV @ECCV 2018</a></li>
                              <li><a href="https://wicvworkshop.github.io/CVPR2018/index.html">WiCV @CVPR 2018</a></li>
                              <li><a href="https://wicvworkshop.github.io/2017/index.html">WiCV 2017</a></li>
                              <li><a href="https://sites.google.com/site/wicv2016/home">WiCV 2016</a></li>
                              <li><a href="https://sites.google.com/site/wicv2015/home">WiCV 2015</a></li>
                           </ul>
                        </li>
                    </ul>
                </nav>
            </div>
        </div>
        
        <div class="banner-wrapper">
            <div class="inner">
                <!-- Banner -->
                <section class="banner container">
                    <br>
                    <h2 id="faculty">Program</h2>
                    
                </section>
            </div>
        </div>
        <div class="features-wrapper">

        <p style="color:red;"><strong>* Below is the final schedule. * </strong></p> 
        <div class="tab">
          <button class="tablinks" onclick="openCity(event, 'Schedule')" id="defaultOpen">Schedule</button>
          <button class="tablinks" onclick="openCity(event, 'Talks')">Talks</button>
          <button class="tablinks" onclick="openCity(event, 'Panel')">Panel</button>
          <button class="tablinks" onclick="openCity(event, 'Orals')">Orals</button>
          <button class="tablinks" onclick="openCity(event, 'Posters')">Posters</button>
          <button class="tablinks" onclick="openCity(event, 'Dinner')">Dinner</button>
        </div>
        
        <div id="Schedule" class="tabcontent">
          <h3 style="margin: 20px 0px 0px 0px;">Main Workshop on September 9, 2018</h3>
          <h5 style="color:red; margin: 0px 0px 20px 0px;">Room: N1090 ZG, Building N1</h5>
          <p style="text-align:left"><b>Note: </b> All ECCV workshops will take place in Technische Universität München (TUM). Find <a href="https://eccv2018.org/attending/venues/">here</a> instructions to reach the venue.</p>  
          <p style="margin: 0px 0px 20px 0px;"></p>
          <p style="margin: 0px 0px 20px 40px;text-align:left;">
                1:00 - 1:20 pm &emsp;&emsp;<b>Lunch bags in the poster area</b> <br>
                1:20 - 1:30 pm &emsp;&emsp;<b>Introduction</b> <br>
                1:30 - 2:00 pm &emsp;&emsp;<b>Keynote</b><br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Vision & Language, by <a href="http://www.tamaraberg.com/">Tamara Berg</a> (UNC Chapel Hill, Shopagon)<br>
                <!-- &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Learning to Segment Moving Objects, by <a href="http://thoth.inrialpes.fr/~schmid/">Cordelia Schmid (INRIA)</a> <br> -->
                2:00 - 2:15 pm &emsp;&emsp;<b>Oral session 1</b><br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Deep Video Color Propagation<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;by Simone Meyer (ETH Zurich)<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Fashion is Taking Shape: Understanding Clothing Preference based on Body Shape from Online Sources<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;by Hosnieh Sattar (Max Planck Institute for Informatics and Saarland University)<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Unsupervised Learning and Segmentation of Complex Activities from Video<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;by Fadime Sener (University of Bonn)<br>
                2:15 - 2:45 pm &emsp;&emsp;<b>Keynote</b><br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Adapting Neural Networks to New Tasks, by <a href="http://slazebni.cs.illinois.edu/">Svetlana Lazebnik</a> (University of Illinois at Urbana-Champaign)  <br>
                2:45 - 4:20 pm &emsp;&emsp;<b>Poster session and coffee break</b><br>
                4:20 - 4:50 pm &emsp;&emsp;<b>Keynote</b><br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Explainable AI Models and Why We Need Them, by <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a> (Boston University)  <br>
                4:50 - 5:00 pm &emsp;&emsp;<b>Oral Session 2</b> <br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Tracking Extreme Climate Events<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;by Sookyung Kim (Lawrence Livermore National Laboratory)<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;A Deep Look into Hand Segmentation<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;by Aisha Urooj (University of Central Florida)<br>
                5:00 - 5:50 pm &emsp;&emsp;<b>Panel session</b><br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<a href="http://www.tamaraberg.com/">Tamara Berg</a> (UNC Chapel Hill, Shopagon)<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<a href="https://www.microsoft.com/en-us/research/people/awf/">Andrew Fitzgibbon</a> (Microsoft HoloLens)<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<a href="http://slazebni.cs.illinois.edu/">Svetlana Lazebnik</a> (University of Illinois at Urbana-Champaign)<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a> (Boston University)<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a> (UC Berkeley)<br>
                5:50 - 6:00 pm &emsp;&emsp;<b>Closing remarks and prizes</b> <br>
                7:00 pm &emsp;&emsp;&emsp;&emsp;&emsp;<b>Apple WiCV Banquet</b> (by invitation)<br>
          </p>
        </div> <!-- Schedule end --> 


        <div id="Talks" class="tabcontent">
          <h2 style="margin: 20px 0px 0px 0px;">Keynote Talks</h2>
          <p>Keynote speakers will give technical talks about their research in computer vision.</p> 
          
        <!-- Tamara Berg --> 
          <div class="row">
            <div class="4u"> 
                <a href="http://www.tamaraberg.com/" class="image image-centered"><img height=230 width=120 src="images/speakers/TamaraBerg.jpg" alt="" /></a>
                <strong> Tamara Berg (UNC Chapel Hill, Shopagon)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                   <b>Title:</b> Vision & Language
                    <br><br>
                    <b>Abstract:</b> TBA

                    <br><br>
                    <b>Bio:</b> 
                    Tamara Berg received her B.S. in Mathematics and Computer Science from the University of Wisconsin, Madison in 2001. She then completed a PhD in Computer Science from the University of California, Berkeley in 2007 under the advisorship of Professor David Forsyth as a member of the Berkeley Computer Vision Group. Afterward, she spent 1 year as a research scientist at Yahoo! Research. From 2008-2013 she was an Assistant Professor in the Computer Science department at Stony Brook University and core member of the consortium for Digital Art, Culture, and Technology (cDACT). She joined the computer science department at the University of North Carolina Chapel Hill (UNC) in Fall 2013 and is currently a tenured Associate Professor. She is the recipient of an NSF Career award, 2 google faculty awards, the 2013 Marr Prize, and the 2016 UNC Hettleman Award.
                  </p> 
              </div>
            </div> 

        <!-- Svetlana Lazebnik --> 
          <div class="row">
            <div class="4u"> 
                <a href="http://slazebni.cs.illinois.edu/" class="image image-centered"><img height=230 width=120 src="images/speakers/SvetlanaLazebnik.jpg" alt="" /></a>
                <strong> Svetlana Lazebnik (University of Illinois at Urbana-Champaign)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                   <b>Title:</b> Adapting Neural Networks to New Tasks
                    <br><br>
                    <b>Abstract:</b> TBA

                    <br><br>
                    <b>Bio:</b> 
                    Svetlana Lazebnik received her Ph.D. at UIUC in May 2006 under the supervision of Prof. Jean Ponce. From August 2007 to December 2011 she was an assistant professor at the University of North Carolina at Chapel Hill, and as of January 2012, she has returned as faculty to U of I. Her research specialty is computer vision. The main themes of her research include scene understanding, joint modeling of images and text, large-scale photo collections, and machine learning techniques for visual recognition problems.
                    Current and former sources of support for her research include the National Science Foundation (under grants IIS 1718221, IIS 1563727, IIS 1228082, CIF 1302438, and IIS 0916829), Microsoft Research Faculty Fellowship, Xerox University Affairs Committee Grants, DARPA Computer Science Study Group, Sloan Foundation Fellowship, Google Research Award, ARO, and Adobe.
                  </p> 
              </div>
            </div> 

        <!-- Kate Saenko --> 
          <div class="row">
            <div class="4u"> 
                <a href="http://ai.bu.edu/ksaenko.html" class="image image-centered"><img height=230 width=120 src="images/speakers/KateSaenko.jpg" alt="" /></a>
                <strong> Kate Saenko (Boston University)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                   <b>Title:</b> Explainable AI Models and Why We Need Them
                    <br><br>
                    <b>Abstract:</b> TBA

                    <br><br>
                    <b>Bio:</b> 
                    Kate Saenko is an Associate Professor of Computer Science at Boston University and director of the Computer Vision and Learning Group. She is also a member of the IVC research group. Her past academic positions include: Assistant Professor at the Computer Science Department at UMass Lowell, Postdoctoral Researcher at the International Computer Science Institute, Visiting Scholar at UC Berkeley EECS and a Visiting Postdoctoral Fellow in the School of Engineering and Applied Science at Harvard University. Her research interests are in the broad area of Artificial Intelligence with a focus on Adaptive Machine Learning, Learning for Vision and Language Understanding, and Deep Learning.
                  </p> 
              </div>
            </div> 

        </div> <!-- Talks end --> 
        

        <div id="Panel" class="tabcontent">
          <h3 style="margin: 20px 0px 0px 0px;">Panel</h3>
          <p>Panelists will answer questions and discuss about increasing diversity in computer vision.</p>
          <p>Feel free to ask your anonymous questions <a href="https://goo.gl/forms/2k2SZFOKIiv3Slnf1">here</a>.</p> 
 
        <!-- Tamara Berg --> 
          <div class="row">
            <div class="4u"> 
                <a href="http://www.tamaraberg.com/" class="image image-centered"><img height=230 width=120 src="images/speakers/TamaraBerg.jpg" alt="" /></a>
                <strong> Tamara Berg (UNC Chapel Hill, Shopagon)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    <b>Bio:</b> 
                    Tamara Berg received her B.S. in Mathematics and Computer Science from the University of Wisconsin, Madison in 2001. She then completed a PhD in Computer Science from the University of California, Berkeley in 2007 under the advisorship of Professor David Forsyth as a member of the Berkeley Computer Vision Group. Afterward, she spent 1 year as a research scientist at Yahoo! Research. From 2008-2013 she was an Assistant Professor in the Computer Science department at Stony Brook University and core member of the consortium for Digital Art, Culture, and Technology (cDACT). She joined the computer science department at the University of North Carolina Chapel Hill (UNC) in Fall 2013 and is currently a tenured Associate Professor. She is the recipient of an NSF Career award, 2 google faculty awards, the 2013 Marr Prize, and the 2016 UNC Hettleman Award.
                  </p> 
              </div>
            </div> 

        <!-- Andrew Fitzgibbon --> 
          <div class="row">
            <div class="4u"> 
                <a href="https://www.microsoft.com/en-us/research/people/awf/" class="image image-centered"><img height=230 width=120 src="images/speakers/AndrewFitzgibbon.jpg" alt="" /></a>
                <strong> Andrew Fitzgibbon (Microsoft HoloLens)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    <b>Bio:</b> 
                    Andrew Fitzgibbon is a scientist with HoloLens at Microsoft, Cambridge, UK. He is best known for his work on 3D vision, having been a core contributor to the Emmy-award-winning 3D camera tracker “boujou” and Kinect for Xbox 360, but his interests are broad, spanning computer vision, graphics, machine learning, and occasionally a little neuroscience. He has published numerous highly-cited papers, and received many awards for his work, including ten “best paper” prizes at various venues, the Silver medal of the Royal Academy of Engineering, and the BCS Roger Needham award. He is a fellow of the Royal Academy of Engineering, the British Computer Society, and the International Association for Pattern Recognition. Before joining Microsoft in 2005, he was a Royal Society University Research Fellow at Oxford University, having previously studied at Edinburgh University, Heriot-Watt University, and University College, Cork.
                  </p> 
              </div>
            </div> 

          <!-- Svetlana Lazebnik --> 
          <div class="row">
            <div class="4u"> 
                <a href="http://slazebni.cs.illinois.edu/" class="image image-centered"><img height=230 width=120 src="images/speakers/SvetlanaLazebnik.jpg" alt="" /></a>
                <strong> Svetlana Lazebnik (University of Illinois at Urbana-Champaign)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    <b>Bio:</b> 
                    Svetlana Lazebnik received her Ph.D. at UIUC in May 2006 under the supervision of Prof. Jean Ponce. From August 2007 to December 2011 she was an assistant professor at the University of North Carolina at Chapel Hill, and as of January 2012, she has returned as faculty to U of I. Her research specialty is computer vision. The main themes of her research include scene understanding, joint modeling of images and text, large-scale photo collections, and machine learning techniques for visual recognition problems.
                    Current and former sources of support for her research include the National Science Foundation (under grants IIS 1718221, IIS 1563727, IIS 1228082, CIF 1302438, and IIS 0916829), Microsoft Research Faculty Fellowship, Xerox University Affairs Committee Grants, DARPA Computer Science Study Group, Sloan Foundation Fellowship, Google Research Award, ARO, and Adobe.
                  </p> 
              </div>
            </div>
 
          <!-- Jitendra Malik -->
          <div class="row">
            <div class="4u">
                <a href="https://people.eecs.berkeley.edu/~malik/" class="image image-centered"><img height=230 width=120 src="images/speakers/JitendraMalik.jpg" alt="" /></a>
                <strong> Jitendra Malik (UC Berkeley)</strong>
              </div>
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    <b>Bio:</b>
                Jitendra Malik was born in Mathura, India in 1960. He received the B.Tech degree in Electrical Engineering from Indian Institute of Technology, Kanpur in 1980 and the PhD degree in Computer Science from Stanford University in 1985. In January 1986, he joined the university of California at Berkeley, where he is currently the Arthur J. Chick Professor in the Department of Electrical Engineering and Computer Sciences. He is also on the faculty of the department of Bioengineering, and the Cognitive Science and Vision Science groups. During 2002-2004 he served as the Chair of the Computer Science Division, and as the Department Chair of EECS during 2004-2006 as well as 2016-2017. Since January 2018, he is also Research Director and Site Lead of Facebook AI Research in Menlo Park.
<br>
Prof. Malik's research group has worked on many different topics in computer vision, computational modeling of human vision, computer graphics and the analysis of biological images. Several well-known concepts and algorithms arose in this research, such as anisotropic diffusion, normalized cuts, high dynamic range imaging, shape contexts and R-CNN. He has mentored more than 60 PhD students and postdoctoral fellows.
<br>
He received the gold medal for the best graduating student in Electrical Engineering from IIT Kanpur in 1980 and a Presidential Young Investigator Award in 1989. At UC Berkeley, he was selected for the Diane S. McEntyre Award for Excellence in Teaching in 2000 and a Miller Research Professorship in 2001. He received the Distinguished Alumnus Award from IIT Kanpur in 2008. His publications have received numerous best paper awards, including five test of time awards - the Longuet-Higgins Prize for papers published at CVPR (twice) and the Helmholtz Prize for papers published at ICCV (three times). He received the 2013 IEEE PAMI-TC Distinguished Researcher in Computer Vision Award, the 2014 K.S. Fu Prize from the International Association of Pattern Recognition, the 2016 ACM-AAAI Allen Newell Award, and the 2018 IJCAI Award for Research Excellence in AI. He is a fellow of the IEEE and the ACM. He is a member of the National Academy of Engineering and the National Academy of Sciences, and a fellow of the American Academy of Arts and Sciences. 
                </p> 
              </div>
            </div>


          <!-- Kate Saenko --> 
          <div class="row">
            <div class="4u"> 
                <a href="http://ai.bu.edu/ksaenko.html" class="image image-centered"><img height=230 width=120 src="images/speakers/KateSaenko.jpg" alt="" /></a>
                <strong> Kate Saenko (Boston University)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    <b>Bio:</b> 
                    Kate Saenko is an Associate Professor of Computer Science at Boston University and director of the Computer Vision and Learning Group. She is also a member of the IVC research group. Her past academic positions include: Assistant Professor at the Computer Science Department at UMass Lowell, Postdoctoral Researcher at the International Computer Science Institute, Visiting Scholar at UC Berkeley EECS and a Visiting Postdoctoral Fellow in the School of Engineering and Applied Science at Harvard University. Her research interests are in the broad area of Artificial Intelligence with a focus on Adaptive Machine Learning, Learning for Vision and Language Understanding, and Deep Learning.
                  </p> 
              </div>
            </div> 

        </div> <!-- Panel end --> 


        <div id="Orals" class="tabcontent">
          <h3 style="margin: 20px 0px 0px 0px;">Oral Presentations</h3>
          <p>A few accepted abstracts are invited to give oral presentations.</p>
          
<p style="text-align:left"><b>Presenter instructions: </b> The presentations should be 4 minute talk and 1 minute Q/A.</p>   
          <div class="divider">
              <hr class="left"/><b>Accepted orals</b><hr class="right" />
          </div>

          <div class="row" style="text-align:left"></div>

          <div class="row" style="text-align:left">
            <div class="1u"> </div>
            <div class="2u"> <b>Presenter Name</b> </div>
            <div class="2u"> <b>Institution</b> </div>
            <div class="7u"> <b>Paper Title</b> </div>
          </div>  

          <hr>
          <div class="row" style="text-align:left"></div>
      
          <div class="row" style="text-align:left">
            <div class="1u"> </div><div class="2u">  Simone Meyer </div>
            <div class="2u">  ETH Zurich  </div>
            <div class="7u">  Deep Video Color Propagation</div>
          </div>

          <div class="row" style="text-align:left">
            <div class="1u"> </div><div class="2u">  Hosnieh Sattar </div>
            <div class="2u">  Max Planck Institute for Informatics and Saarland University  </div>
            <div class="7u">  Fashion is Taking Shape: Understanding Clothing Preference based on Body Shape from Online Sources</div>
          </div>

          <div class="row" style="text-align:left">
            <div class="1u"> </div><div class="2u">  Fadime Sener,  </div>
            <div class="2u">  University of Bonn  </div>
            <div class="7u">  Unsupervised Learning and Segmentation of Complex Activities from Video</div>
          </div>

          <div class="row" style="text-align:left">
            <div class="1u"> </div><div class="2u">  Sookyung Kim  </div>
            <div class="2u">  Lawrence Livermore National Laboratory  </div>
            <div class="7u">  Tracking Extreme Climate Events</div>
          </div>

          <div class="row" style="text-align:left">
            <div class="1u"> </div><div class="2u">  Aisha Urooj  </div>
            <div class="2u">  University of Central Florida  </div>
            <div class="7u">  A Deep Look into Hand Segmentation</div>
          </div>

        </div> <!-- Orals end --> 


        <div id="Posters" class="tabcontent">
          <h3 style="margin: 20px 0px 0px 0px;">Poster Presentations</h3>
          <p>Authors of all accepted abstracts will present their work in a poster session.</p>                
          
          <p style="text-align:left"><b>Presenter instructions:</b>
            Please note your poster number below to find your board. All posters should be installed in at most 10 minutes at the start of the poster session. The posters have to be in portrait mode. The poster boards are 1.20x1.00 meters and cannot hold landscape posters.
          </p>

          <div class="divider">
              <hr class="left"/><b>Accepted Posters</b><hr class="right" />
          </div>

          <br>

          <div class="row" style="text-align:left">
            <div class="1u"> <b>No</b></div>
            <div class="2u"> <b>Presenter Name</b> </div>
            <div class="2u"> <b>Institution</b> </div>
            <div class="7u"> <b>Paper Title</b> </div>
          </div>

          <hr>
          <br>

            <div class="row" style="text-align:left">
              <div class="1u"> 1 </div>
              <div class="2u"> Maryam Babaee </div>
              <div class="2u"> Chair of Human-Machine Communication, TU Munich, Germany</div>
              <div class="7u"> Gait Energy Image Reconstruction from Degraded Gait Cycle Using Deep Learning  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 2 </div>
              <div class="2u"> Noelia Vallez </div>
              <div class="2u"> UCLM</div>
              <div class="7u"> Deep learning for the Eyes of Things platform  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 3 </div>
              <div class="2u"> Clara Fernandez Labrador </div>
              <div class="2u"> University of Zaragoza</div>
              <div class="7u"> Full 3D Layout Reconstruction from One Single 360º Image  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 4 </div>
              <div class="2u"> Melani Sanchez </div>
              <div class="2u"> University of Zaragoza</div>
              <div class="7u"> Smart Representation of Indoor Scenes under Simulated Prosthetic Vision  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 5 </div>
              <div class="2u"> Rania Briq </div>
              <div class="2u"> The University of Bonn</div>
              <div class="7u"> Convolutional Simplex Projection Network for Weakly Supervised Semantic Segmentation  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 6 </div>
              <div class="2u"> Farzaneh Mahdisoltani </div>
              <div class="2u"> University of Toronto</div>
              <div class="7u"> Hierarchical Video Understanding  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 7 </div>
              <div class="2u"> Alina Roitberg </div>
              <div class="2u"> KIT</div>
              <div class="7u"> Towards Human Activity Recognition in Autonomous Vehicles  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 8 </div>
              <div class="2u"> Míriam Bellver </div>
              <div class="2u"> Barcelona Supercomputing Center</div>
              <div class="7u"> From Pixels to Object Sequences: Recurrent Semantic Instance Segmentation  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 9 </div>
              <div class="2u"> Sara Elkerdawy </div>
              <div class="2u"> University of Alberta</div>
              <div class="7u"> Fine-Grained Vehicle Classification with Unsupervised Parts Features Learning  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 10 </div>
              <div class="2u"> Aina Ferrà Marcús </div>
              <div class="2u"> Universitat de Barcelona</div>
              <div class="7u"> Multiple Wavelet Pooling for CNNs  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 11 </div>
              <div class="2u"> Jhan Alarifi </div>
              <div class="2u"> Manchester Metropolitan University</div>
              <div class="7u"> Automated Facial Wrinkles Annotator  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 12 </div>
              <div class="2u"> Nazanin Mehrasa </div>
              <div class="2u"> Simon Fraser University</div>
              <div class="7u"> Learning Trajectory Representations for Human Activity Analysis  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 13 </div>
              <div class="2u"> Mengyao Zhai </div>
              <div class="2u"> Simon Fraser University</div>
              <div class="7u"> Deep Learning of Appearance Models for Online Object Tracking  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 14 </div>
              <div class="2u"> Nazre Batool </div>
              <div class="2u"> Scania CV AB</div>
              <div class="7u"> Real-time Recognition of Turn and Brake Signals for Autonomous Urban Buses  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 15 </div>
              <div class="2u"> Jingyuan Liu </div>
              <div class="2u"> Fudan University</div>
              <div class="7u"> Deep Fashion Analysis with Feature Map Upsampling and Landmark-driven Attention  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 16 </div>
              <div class="2u"> Marcella Cornia </div>
              <div class="2u"> University of Modena and Reggio Emilia</div>
              <div class="7u"> Towards Cycle-Consistent Models for Text and Image Retrieval  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 17 </div>
              <div class="2u"> Antitza Dantcheva </div>
              <div class="2u"> INRIA</div>
              <div class="7u"> From attribute-labels to faces: text based face generation using a conditional generative adversarial network  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 18 </div>
              <div class="2u"> Bingbin Liu </div>
              <div class="2u"> Stanford University</div>
              <div class="7u"> Temporal Modular Networks for Retrieving Complex Compositional Activities in Video  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 19 </div>
              <div class="2u"> Kanami Yamagishi </div>
              <div class="2u"> Waseda University</div>
              <div class="7u"> How Do Computers See Makeup?: Investigating the Effects of Makeup on 3D Facial Reconstruction  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 20 </div>
              <div class="2u"> Simone Meyer </div>
              <div class="2u"> ETH Zurich</div>
              <div class="7u"> Deep Video Color Propagation  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 21 </div>
              <div class="2u"> Hosnieh Sattar </div>
              <div class="2u"> Max Planck Institute for Informatics and Saarland University</div>
              <div class="7u"> Fashion is Taking Shape: Understanding Clothing Preference based on Body Shape from Online Sources  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 22 </div>
              <div class="2u"> Jadisha Ramirez Cornejo </div>
              <div class="2u"> University of Campinas</div>
              <div class="7u"> Dynamic Facial Expression Recognition by means of Visual Rhythm and Motion History Image  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 23 </div>
              <div class="2u"> Sandra Aigner </div>
              <div class="2u"> Technical University of Munich</div>
              <div class="7u"> FakeFutureGAN: Generating the Future using Spatio-Temporal 3d Convolutions in Progressively Growing GANs  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 24 </div>
              <div class="2u"> Obioma Pelka </div>
              <div class="2u"> University of Applied Sciences and Arts Dortmund</div>
              <div class="7u"> Optimizing Body Region Classification With Deep Convolutional Activation Features  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 25 </div>
              <div class="2u"> Uldanay Bairam </div>
              <div class="2u"> NTNU</div>
              <div class="7u"> Highlight removal from fruit images for improved quality control and digital phenotyping  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 26 </div>
              <div class="2u"> Fadime Sener </div>
              <div class="2u"> University of Bonn</div>
              <div class="7u"> Unsupervised Learning and Segmentation of Complex Activities from Video  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 27 </div>
              <div class="2u"> Leissi Margarita Castaneda Leon </div>
              <div class="2u"> University of São Paulo</div>
              <div class="7u"> Efficient Interactive Multi-Object Segmentation in Medical Images  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 28 </div>
              <div class="2u"> Michelle Guo </div>
              <div class="2u"> Stanford University</div>
              <div class="7u"> Neural Graph Matching Networks for Fewshot 3D Action Recognition  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 29 </div>
              <div class="2u"> Deniz Engin </div>
              <div class="2u"> Istanbul Technical University</div>
              <div class="7u"> Face Frontalization for Cross-Pose Facial Expression Recognition  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 30 </div>
              <div class="2u"> Michelle Guo </div>
              <div class="2u"> Stanford University</div>
              <div class="7u"> Focus on the Hard Things: Dynamic Task Prioritization for Multitask Learning  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 31 </div>
              <div class="2u"> Amanda Duarte </div>
              <div class="2u"> Universitat Politecnica de Catalunya</div>
              <div class="7u"> Cross-modal Embeddings for Video and Audio Retrieval  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 32 </div>
              <div class="2u"> Sookyung Kim </div>
              <div class="2u"> Lawrence Livermore National Laboratory</div>
              <div class="7u"> Tracking Extreme Climate Events  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 33 </div>
              <div class="2u"> Pallabi Ghosh </div>
              <div class="2u"> University of Maryland</div>
              <div class="7u"> Understanding Center Loss Based Network for Image Retrieval with Few Training Data  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 34 </div>
              <div class="2u"> Yasemin Timar </div>
              <div class="2u"> Bogazici University</div>
              <div class="7u"> Stylistic Features in Affective Movie Analysis  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 35 </div>
              <div class="2u"> Aisha Urooj </div>
              <div class="2u"> University of Central Florida</div>
              <div class="7u"> A Deep Look into Hand Segmentation  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 36 </div>
              <div class="2u"> Rafia Rahim </div>
              <div class="2u"> National University of computer and emerging sciences, islamabad</div>
              <div class="7u"> End-to-End Trained CNN Encoder-Decoder Networks for Image Steganography  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 37 </div>
              <div class="2u"> Shreya Hasmukh Patel </div>
              <div class="2u"> IIT Jodhpur</div>
              <div class="7u"> Cancellable knuckle template generation based on LBP-CNN  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 38 </div>
              <div class="2u"> Ruba Alkadi </div>
              <div class="2u"> Khalifa university</div>
              <div class="7u"> A 2.5D Deep Learning-based Approach For Prostate Cancer Detection on T2-weighted Magnetic Resonance Imaging  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 39 </div>
              <div class="2u"> Geethu Jacob </div>
              <div class="2u"> Indian Institute of Technology, Chennai</div>
              <div class="7u"> GreenWarps: A Two-Stage Warping Model for Stitching Images using Diffeomorphic Meshes and Green Coordinates  </div>
            </div>
            <div class="row" style="text-align:left">
              <div class="1u"> 40 </div>
              <div class="2u"> Anjali Chadha </div>
              <div class="2u"> Texas A&M University</div>
              <div class="7u"> Enabling Pedestrian Safety using Computer Vision Techniques: A Case Study of the 2018 Uber Inc. Self-driving Car Crash  </div>
            </div>



        </div> <!-- Posters end --> 


        <div id="Dinner" class="tabcontent">
          <h3 style="margin: 20px 0px 0px 0px;">Mentoring Dinner on September 9, 2018</h3>
          <p style="font-style:italic;">by invitation only</p>
          <p>Dinner sponsored by Apple</p>
          <p align="left";>The dinner event is an opportunity to meet other female computer vision researchers. Poster presenters will be matched with senior computer vision researchers to share experience and career advice. Invitees will receive an e-mail and be asked to confirm attendance.</p>

          <div class="divider">
              <hr class="left"/><b>Dinner speakers</b><hr class="right" />
          </div><br>

          <!-- Megan Maher --> 
          <div class="row">
            <div class="4u"> 
                <a href="" class="image image-centered"><img height=230 width=120 src="images/speakers/MeganMaher.jpg" alt="" /></a>
                <strong> Megan Maher (Apple)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    <b>Bio:</b> 
                    Megan Maher is an AI researcher at Apple. Her research interests include machine learning, computer vision, and robotics. She received her Bachelor’s degree from Bowdoin College in Brunswick, Maine, where she majored in computer science and mathematics, achieving the highest distinction in the computer science program. Before joining Apple, she was a co-captain of Bowdoin’s RoboCup team, the Northern Bites, which placed internationally as one of the only all-undergraduate teams in the Standard Platform League.  Pursuing her passion to support underrepresented groups in STEM, she mentors in the local community and serves on a leadership team to support women at Apple. She is a recipient of various research awards including the Clare Booth Luce Fellowship, the Alan B. Tucker Computer Science Research Award, the Maine Space Grant Consortium Summer Fellowship, and the National Science Foundation Summer Fellowship.
                  </p> 
              </div>
            </div> 

          <!-- Tinne Tuytelaars --> 
          <div class="row">
            <div class="4u"> 
                <a href="http://homes.esat.kuleuven.be/~tuytelaa/" class="image image-centered"><img height=230 width=120 src="images/speakers/TinneTuytelaars.jpg" alt="" /></a>
                <strong> Tinne Tuytelaars (KU Leuven)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    <b>Bio:</b> 
                    Tinne Tuytelaars received a Master of Electrical Engineering from the  K.U. Leuven , Belgium in 1996. Since her graduation, she has been working at the  VISICS -lab within  ESAT - PSI   of the Katholieke Universiteit Leuven , where she defended her PhD on december 19, 2000, entitled "Local Invariant Features for Registration and Recognition".
During most of 2006 and early 2007, she was also parttime (20%) visiting scientist at the LEAR group of INRIA in Grenoble. Summer 2008, she visited the Making Sense from Data group at NICTA in Canberra, Australia. Summer 2010 she visited Trevor Darrell's group at ICSI/EECS UC Berkeley.
Since October 2008, she is appointed research professor (BOF-ZAP) at the KU Leuven.
                  </p> 
              </div>
            </div> 

          <!-- Raquel Urtasun --> 
          <div class="row">
            <div class="4u"> 
                <a href="http://www.cs.toronto.edu/~urtasun/" class="image image-centered"><img height=230 width=120 src="images/speakers/RaquelUrtasun.jpg" alt="" /></a>
                <strong> Raquel Urtasun (University of Toronto, Uber ATG)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    <b>Bio:</b> 
                    Raquel Urtasun is the Head of Uber ATG Toronto. She is also an Associate Professor in the Department of Computer Science at the University of Toronto, a Canada Research Chair in Machine Learning and Computer Vision and a co-founder of the Vector Institute for AI. She received her Ph.D. degree from the Computer Science department at Ecole Polytechnique Federal de Lausanne (EPFL) in 2006 and did her postdoc at MIT and UC Berkeley. She is a world leading expert in machine perception for self-driving cars. Her research interests include machine learning, computer vision, robotics and remote sensing. Her lab was selected as an NVIDIA NVAIL lab. She is a recipient of an NSERC EWR Steacie Award, an NVIDIA Pioneers of AI Award, a Ministry of Education and Innovation Early Researcher Award, three Google Faculty Research Awards, an Amazon Faculty Research Award, a Connaught New Researcher Award and two Best Paper Runner up Prize awarded at CVPR in 2013 and 2017 respectively.
                  </p> 
              </div>
            </div> 


        </div> <!-- Dinner end --> 

        <script>
           function openCity(evt, cityName) {
               var i, tabcontent, tablinks;
               tabcontent = document.getElementsByClassName("tabcontent");
               for (i = 0; i < tabcontent.length; i++) {
                   tabcontent[i].style.display = "none";
               }
               tablinks = document.getElementsByClassName("tablinks");
               for (i = 0; i < tablinks.length; i++) {
                   tablinks[i].className = tablinks[i].className.replace(" active", "");
               }
               document.getElementById(cityName).style.display = "block";
               evt.currentTarget.className += " active";
           }
           
           // Get the element with id="defaultOpen" and click on it
           document.getElementById("defaultOpen").click();
        </script>

          <br><br><br><br><br> 
        </div>
        <!-- Footer Wrapper -->
        <div class="footer">
            <div class="container">
                    <div class="row">
                            <div class="7u">
             <p>&copy; WiCV @ECCV 2018</p>
            </div>
            <div class="1u">
                   <a href="mailto:wicv-eccv18-organizers@googlegroups.com" class="fa fa-envelope" style="font-size:1.25em;" data-placement="top" data-toggle="tooltip" title="Email"></a>
                       </div>
                       <div class="1u">                       
                                                        <a href="https://twitter.com/wicvworkshop" class="fa fa-twitter" data-placement="top" style="font-size:1.25em;"  data-toggle="tooltip" title="Twitter"></a>
                                                     </div>
                                                         <div class="1u">
                                                               <a href="https://www.facebook.com/WomenInComputerVision/" class="fa fa-facebook" data-placement="top" style="font-size:1.25em;" data-toggle="tooltip" title="Facebook"></a>
                                                               </div>
                                                           </div>
                                                           </div>
                                                         </div>
        <script src="//static.getclicky.com/js" type="text/javascript"></script>
        <script type="text/javascript">try{ clicky.init(100926441); }catch(e){}</script>
        <noscript><p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/100926441ns.gif" /></p></noscript>
    </body>
</html>
