---
# this is an empty front matter
---
<!DOCTYPE HTML>


<!--
 Berkeley Vision and Learning Center (BVLC)
 
 Design based on:
 Strongly Typed 1.0 by HTML5 UP
 html5up.net | @n33co
 Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
 -->
<html>
    <head>
         {% include_relative _includes/header.html %}
        <style>
        
        /* Style the tab */
        div.tab {
            margin: auto;
            width: 75%;
            padding: 0px 12px; 
            overflow: hidden;
            border: 1px solid #ccc;
            background-color: #f1f1f1;
        }
        
        /* Style the buttons inside the tab */
        div.tab button {
            background-color: inherit;
            float: left;
            border: none;
            outline: none;
            cursor: pointer;
            padding: 14px 16px;
            transition: 0.3s;
            font-size: 17px;
        }
        
        /* Change background color of buttons on hover */
        div.tab button:hover {
            background-color: #ddd;
        }
        
        /* Create an active/current tablink class */
        div.tab button.active {
            background-color: #ccc;
        }
        
        /* Style the tab content */
        .tabcontent {
            margin: auto;
            width: 75%;
            display: none;
            padding: 6px 12px;
            border: 1px solid #ccc;
            border-top: none;
        }
        </style>

            </head>
    <body class="homepage">
        <!-- Header Wrapper -->
         {% include_relative _includes/menu.html %}
        
        <div class="banner-wrapper">
            <div class="inner">
                <!-- Banner -->
                <section class="banner container">
                    <br>
                    <h2 id="faculty">Program</h2>
                    
                </section>
            </div>
        </div>
        <div class="features-wrapper">

            <div class="tab">
              <button class="tablinks" onclick="openCity(event, 'Schedule')" id="defaultOpen">Schedule</button>
              <button class="tablinks" onclick="openCity(event, 'Talks')">Talks</button>
              <button class="tablinks" onclick="openCity(event, 'Panel')">Panel</button>
              <button class="tablinks" onclick="openCity(event, 'Orals')">Orals</button>
              <button class="tablinks" onclick="openCity(event, 'Posters')">Posters</button>
              <button class="tablinks" onclick="openCity(event, 'Dinner')">Dinner</button>
            </div>

            <div id="Schedule" class="tabcontent">
              <h2 style="margin: 20px 0px 0px 0px;">Main Workshop on June 16, 2019</h2>

                <hr class="left"/><b>Schedule</b><hr class="right" />
          <br>


                <p style="margin: 0px 0px 20px 10px;">Click on any session in the calendar to get more information about the speakers and the description of the talks.</p>
                <div class="embed-container">
                <iframe src="https://calendar.google.com/calendar/embed?showNav=0&amp;showDate=0&amp;showPrint=0&amp;showTabs=0&amp;showCalendars=0&amp;mode=agenda&dates=20190616/20190617&amp;height=400&amp;wkst=1&amp;hl=en&amp;bgcolor=%23ccccff&amp;src=2esb2ru5l09v5fqckm8ijol0s0%40group.calendar.google.com&amp;color=%2323164E&amp;ctz=America%2FLos_Angeles" style="border-width:0" width="300" height="300" align="middle" frameborder="0" scrolling="no"></iframe>
                </div>
 <br> <br> <br> <br>
              <hr class="left"/><b>Location</b><hr class="right" />
          <br> <br>

                <div class="embed-container">
                <iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d3316.82438829849!2d-118.19152278444471!3d33.765203580685!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x0%3A0x290d592001d0639!2sLong+Beach+Convention+Center!5e0!3m2!1ses!2ses!4v1558457578350!5m2!1ses!2ses" width="300" height="200" align="middle" frameborder="0" style="border:0" allowfullscreen></iframe>
                </div>
                    <!--<h5 style="color:red;font-style:italic; margin: 0px 0px 20px 0px;">tentative schedule</h5> -->
              <h5 style="color:red; margin: 0px 0px 20px 0px;">Room: Hyatt Regency A</h5>
			  <div class="embed-container">
                <embed src="images/venue/location.pdf" width="400px" height="500px" />
				</div>


            </div>

            <div id="Talks" class="tabcontent">
          <h2 style="margin: 20px 0px 0px 0px;">Invited Talks</h2>
          <p>Invited speakers will give technical talks about their research in computer vision. The invited talks will each have a duration of 25 minutes (20 minute talk and 5 minute Q/A).</p>
			
			
		<div class="row">
            <div class="4u">
                <a href="https://www.cc.gatech.edu/~judy/" class="image image-centered"><img height=230 width=120 src="images/speakers/hoffman.jpg" alt="" /></a>
                <strong>Judy Hoffman (FAIR)</strong>
              </div>
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                   <b>Title:</b> Adversarial robustness and domain adaptation
                    <br><br>
                </p>
              </div>
             <br><br>
            </div>

          <div class="row">
            <div class="4u">
                <a href="http://www.cs.toronto.edu/~urtasun/" class="image image-centered"><img height=230 width=120 src="images/speakers/urtasun.jpg" alt="" /></a>
                <strong> Raquel Urtasun (University of Toronto / Uber ATG)</strong>
              </div>
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                   <b>Title:</b> A future with affordable self-driving vehicles
                    <br><br>
                </p>
              </div>
              <br><br>
            </div>
			
		<div class="row">
            <div class="4u">
                <a href="http://people.csail.mit.edu/klbouman/" class="image image-centered"><img height=230 width=120 src="images/speakers/bouman.jpg" alt="" /></a>
                <strong>Katie Bouman (Caltech)</strong>
              </div>
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                   <b>Title:</b>  Imaging the Unseen: Taking the First Picture of a Black Hole
                    <br><br>
                </p>
              </div>
              <br><br>
            </div>
			
          <div class="row">
            <div class="4u">
                <a href="https://www.cc.gatech.edu/~parikh/" class="image image-centered"><img height=230 width=120 src="images/speakers/deviparikh.jpg" alt="" /></a>
                <strong> Devi Parikh (Georgia Tech / FAIR)</strong>
              </div>
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                   <b>Title:</b> Forcing Vision + Language Models To Actually See, Not Just Talk
                    <br><br>
                </p>
              </div>
              <br><br>
            </div>

          

         


        </div> <!-- Talks end -->

            <div id="Panel" class="tabcontent">
          <h2 style="margin: 20px 0px 0px 0px;">Panel</h2>
          <p>Panelists will answer questions and discuss about increasing diversity in computer vision.</p>
          <p>Feel free to ask your anonymous questions <a href="https://docs.google.com/forms/d/e/1FAIpQLSeGRGQPbAV-dwHbfxdbIbfDMjWmdJA87p5SPGFRbp-1nK0Jfg/viewform">here</a>.</p>

                <section id="features" class="container" style="padding-top:30px;">
              <div class="row">

                  <div class="4u">
                        <section>
                          <div>
                              <a href="https://www.cc.gatech.edu/~parikh/" class="image image-centered"><img height=230 width=120 src="images/speakers/deviparikh.jpg" alt="" /></a>
                                <header><strong>Devi Parikh</strong><p>Georgia Tech / FAIR</p></header>
                          </div>
                        </section>
                  </div>

                  <div class="4u">
                        <section>
                          <div>
                              <a href="https://www.cc.gatech.edu/~judy/" class="image image-centered"><img height=230 width=120 src="images/speakers/hoffman.jpg" alt="" /></a>
                                <header><strong>Judy Hoffman</strong><p>FAIR</p></header>
                          </div>
                        </section>
                  </div>

                  <div class="4u">
                        <section>
                          <div>
                              <a href="https://people.eecs.berkeley.edu/~trevor/" class="image image-centered"><img height=230 width=120 src="images/speakers/darrell.jpg" alt="" /></a>
                                <header><strong>Trevor Darrell</strong><p>UC Berkeley</p></header>
                          </div>
                        </section>
                  </div>


                  <div class="4u">
                        <section>
                          <div>
                              <a href="https://billf.mit.edu/" class="image image-centered"><img height=230 width=120 src="images/speakers/freeman.jpg" alt="" /></a>
                                <header><strong>William T. Freeman</strong><p>MIT / Google</p></header>
                          </div>
                        </section>
                  </div>

                  <div class="4u">
                        <section>
                          <div>
                              <a href="http://www.cs.toronto.edu/~urtasun/" class="image image-centered"><img height=230 width=120 src="images/speakers/urtasun.jpg" alt="" /></a>
                                <header><strong>Raquel Urtasun</strong><p>University of Toronto / Uber ATG</p></header>
                          </div>
                        </section>
                  </div>

                  <div class="4u">
                        <section>
                          <div>
                              <a href="http://people.csail.mit.edu/klbouman/" class="image image-centered"><img height=230 width=120 src="images/speakers/bouman.jpg" alt="" /></a>
                                <header><strong>Katie Bouman</strong><p>Caltech</p></header>
                          </div>
                        </section>
                  </div>






                </div>
                </section>
            </div>

            <div id="Orals" class="tabcontent">
          <h2 style="margin: 20px 0px 0px 0px;">Oral Presentations</h2>
         <p>A few accepted papers are invited to give oral presentations. The presentations will each have a duration of 7.5 minutes (5 minute spotlight and 2.5 minute Q/A).</p>

          <div class="row" style="text-align:left"></div>
          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u"> <b>Presenter Name</b> </div><div class="2u">  <b>Institution</b> </div><div class="7u">  <b>Paper Title</b> </div></div>

          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  Sara Iodice </div><div class="2u">  Imperial College  </div><div class="7u">  Partial Person Re-identification with Alignment and Hallucination</div></div>

          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">    </div><div class="2u">    </div><div class="7u">     <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    <b>Abstract:</b> Partial person re-identification involves matching pedestrian views where only a part of a body is visible in corresponding images. This reflects practical CCTV surveillance scenario, where full person views are often unavailable. Missing body parts make the comparison very challenging due to significant misalignment and varying scale of the views. We propose Partial Matching Net (PMN) that detects body joints, aligns partial views and hallucinates the missing parts based on the information present in the frame and a learned model of a person. The aligned and reconstructed views are then combined into a joint representation and used for matching images. We evaluate our approach and compare to other methods on three different datasets, demonstrating significant improvements.</p>
</div></div>






          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  Parita Pooj  </div><div class="2u">  Columbia University  </div><div class="7u">  The Minimalist Camera  </div></div>

          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">    </div><div class="2u">    </div><div class="7u">     <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    <b>Abstract:</b> We present the minimalist camera (mincam), a design framework to capture the scene information with minimal resources and without constructing an image. The basic sensing unit of a mincam is a ‘mixel’ — an optical photo-detector that aggregates light from the entire scene linearly modulated by a static mask. We precompute a set of masks for a configuration of few mixels, such that they retain minimal information relevant to a task. We show how tasks such as tracking moving objects or determining a vehicle’s speed can be accomplished with a handful of mixels as opposed to the more than a million pixels used in traditional photography. Since mincams are passive, compact, low powered and inexpensive, they can potentially find applications in a broad range of scenarios.</p>
</div></div>



          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  Miriam Bellver </div><div class="2u"> Barcelona Supercomputing Center </div><div class="7u">  Budget-aware Semi-Supervised Semantic and Instance Segmentation </div></div>

          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">    </div><div class="2u">    </div><div class="7u">     <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    <b>Abstract:</b> Methods that move towards less supervised scenarios are key for image segmentation, as dense labels demand significant human intervention. Generally, the annotation burden is mitigated by labeling datasets with weaker forms of supervision, e.g. image-level labels or bounding boxes. Another option are semi-supervised settings, that commonly leverage a few strong annotations and a huge number of unlabeled/weakly-labeled data. In this paper, we revisit semi-supervised segmentation schemes and narrow down significantly the annotation budget (in terms of total labeling time of the training set) compared to previous approaches. With a very simple pipeline, we demonstrate that at low annotation budgets, semi-supervised methods outperform by a wide margin weakly-supervised ones for both semantic and instance segmentation. Our approach also outperforms previous semi-supervised works at a much reduced labeling cost. We present results for the Pascal VOC benchmark and unify weakly and semi-supervised approaches by considering the total annotation budget, thus allowing a fairer comparison between methods.</p>
</div></div>


          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  Sadaf Gulshad </div><div class="2u">  University of Amsterdam </div><div class="7u">  Interpreting Adversarial Examples Using Attributes </div></div>


          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">    </div><div class="2u">    </div><div class="7u">     <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    <b>Abstract:</b> Image classification serves as a fundamental part of many computer vision tasks such as detection, segmentation, captioning, in which deep neural networks excel. But the vulnerability of these models to imperceptible carefully crafted noise have raised questions regarding their robustness. In this work, we open these neural networks based black boxes for adversarial examples by leveraging side information (attributes). We predict attributes for clean as well as adversarial images and analyze how the attributes change when the input is slightly perturbed. We present comprehensive experiments for attribute prediction, adversarial example generation, adversarially robust learning, their qualitative and quantitative analysis using predicted attributes on Caltech-UCSD Birds datasets.</p>
</div></div>


          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  Patsorn Sangkloy </div><div class="2u">  Georgia Institute of Technology </div><div class="7u">  Generate Semantically Similar Images with Kernel Mean Matching  </div></div>


          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">    </div><div class="2u">    </div><div class="7u">     <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    <b>Abstract:</b> We propose a novel procedure which adds "content-addressability" to any given unconditional implicit model e.g., a generative adversarial network (GAN). The procedure allows users to control the generative process by specifying a set (arbitrary size) of desired examples based on which similar samples are generated from the model. The proposed approach, based on kernel mean matching, is applicable to any generative models which transform latent vectors to samples, and does not require retraining of the model. Experiments on various high-dimensional image generation problems (CelebA-HQ, LSUN bedroom, bridge, tower) show that our approach is able to generate images which are consistent with the input set, while retaining the image quality of the original model. To our knowledge, this is the first work that attempts to construct, at test time, a content-addressable generative model from a trained marginal model.
</p>
</div></div>


          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  Xiaodan Hu </div><div class="2u">  University of Waterloo </div><div class="7u"> RUNet: A Robust Architecture for Image Super-Resolution </div></div>


          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">    </div><div class="2u">    </div><div class="7u">     <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    <b>Abstract:</b> Single image super-resolution (SISR) is a challenging ill-posed problem which aims to restore or infer a high-resolution image from a low-resolution one. Powerful deep learning-based techniques have achieved state-of-the-art performance in SISR; however, they can underperform when handling images with non-stationary degradations, such as for the application of projector resolution enhancement. In this paper, a new UNet architecture that is able to learn the relationship between a set of degraded low-resolution images and their corresponding original high-resolution images is proposed. We propose employing a degradation model on training images in a non-stationary way, allowing the construction of a robust UNet (RUNet) for image super-resolution (SR). Experimental results show that the proposed RUNet improves the visual quality of the obtained super-resolution images while maintaining a low reconstruction error.</p>
</div></div>



        </div>

            <div id="Posters" class="tabcontent">
          <h2 style="margin: 20px 0px 0px 0px;">Poster Presentations</h2>
                <p>Authors of all accepted papers will present their work in a poster session. All posters should be installed in at most 10 minutes at the start of the poster session.</p>
               <p> Use your Poster ID to find your poster board at the conference venue.</p>
                <p> The physical dimensions of the poster stands are 8 feet wide by 4 feet high. </p>
                   <p>Poster presenters can optionally use the <a href="http://cvpr2019.thecvf.com/files/cvpr19_poster_template.pptx">CVPR19 poster template</a> for more details on how to prepare their posters. </p>
                <p><strong>The list of accepted papers along with their Poster IDs can be found <a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRLkCPp-zRtzh1OZmi8VE_204xQ8IP5N2kftZBf6W4nzwbPsTrG3k30Kppfj_nI2cHOZc-N_f-8D-Lh/pubhtml?gid=0&single=true">here</a>.</strong></p>

  </div>

            <div id="Dinner" class="tabcontent">
          <h2 style="margin: 20px 0px 0px 0px;">Mentoring Dinner on June 16</h2>
          <!-- <p style="font-style:italic;">by invitation only</p> -->
          <p>7:00 - 10:00 pm   Dinner sponsored by  <strong> <font color="dodgerblue">Google</font></strong> </p>
          <p align="left";>The dinner event is an opportunity to meet other female computer vision researchers. Authors will be matched with senior computer vision researchers to share experience and career advice. Invitees will receive an e-mail and be asked to confirm attendance.</p>

            <hr class="left"/><b>Dinner speakers</b><hr class="right" />
                <br>


                <section id="features" class="container" style="padding-top:30px;">
              <div class="row">

                  <div class="4u">
                        <section>
                          <div>
                              <a href="http://tensorlab.cms.caltech.edu/users/anima/" class="image image-centered"><img height=230 width=120 src="images/speakers/anima.jpg" alt="" /></a>
                                <header><strong>Anima Anandkumar</strong><p>Caltech / NVIDIA</p></header>
                          </div>
                        </section>
                  </div>

                  <div class="4u">
                        <section>
                          <div>
                              <a href="https://people.eecs.berkeley.edu/~kanazawa/" class="image image-centered"><img height=230 width=120 src="images/speakers/angjoo.png" alt="" /></a>
                                <header><strong>Angjoo Kanazawa</strong><p>UC Berkeley</p></header>
                          </div>
                        </section>
                  </div>

                  <div class="4u">
                        <section>
                          <div>
                              <a href="https://thoth.inrialpes.fr/~schmid/" class="image image-centered"><img height=230 width=120 src="images/speakers/schmid.jpg" alt="" /></a>
                                <header><strong>Cordelia Schmid</strong><p>INRIA / Google</p></header>
                          </div>
                        </section>
                  </div>
                </div>
                </section>
                 <br> <br> <br> <br>
              <hr class="left"/><b>Location</b><hr class="right" />
          <br> <br>
                <div class="embed-container">
                <iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d3317.028322306086!2d-118.19407468447616!3d33.759933980686526!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x80dd3125a85f8ef3%3A0x13a546608196176c!2sParkers&#39;+Lighthouse!5e0!3m2!1ses!2ses!4v1558375760565!5m2!1ses!2ses" width="300" height="200" frameborder="0" align="middle" style="border:0" allowfullscreen></iframe>
                </div>
                    <br><br>



            </div>


        <script>
           function openCity(evt, cityName) {
               var i, tabcontent, tablinks;
               tabcontent = document.getElementsByClassName("tabcontent");
               for (i = 0; i < tabcontent.length; i++) {
                   tabcontent[i].style.display = "none";
               }
               tablinks = document.getElementsByClassName("tablinks");
               for (i = 0; i < tablinks.length; i++) {
                   tablinks[i].className = tablinks[i].className.replace(" active", "");
               }
               document.getElementById(cityName).style.display = "block";
               evt.currentTarget.className += " active";
           }
           
           // Get the element with id="defaultOpen" and click on it
           document.getElementById("defaultOpen").click();
        </script>

          <br><br><br><br><br> 
        </div>


        <!-- Footer Wrapper -->
         {% include_relative _includes/footer.html %}
    </body>
</html>
